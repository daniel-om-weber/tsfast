{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07369d3d",
   "metadata": {},
   "source": [
    "# Example 05: Loss Functions and Metrics\n",
    "\n",
    "The choice of loss function and evaluation metrics significantly affects\n",
    "training behavior and how you assess model quality. TSFast provides\n",
    "specialized losses and metrics for time series system identification:\n",
    "\n",
    "- **Loss modifiers** that wrap any base loss to skip transients, slice\n",
    "  windows, or normalize scales\n",
    "- **Evaluation metrics** standard in the system identification literature\n",
    "\n",
    "This example walks through each one and explains when to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b6d59",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This example builds on concepts from Examples 00-02. Make sure you have\n",
    "completed those first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640bc280",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from tsfast.datasets.benchmark import create_dls_silverbox\n",
    "from tsfast.models.rnn import RNNLearner\n",
    "from tsfast.learner.losses import (\n",
    "    fun_rmse, nrmse, nrmse_std, mean_vaf,\n",
    "    weighted_mae, NormLoss, SkipNLoss, CutLoss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95974b0e",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We use the Silverbox benchmark throughout this example. Each model trains for\n",
    "only 5 epochs to keep things fast -- the focus here is on the loss and metric\n",
    "behavior, not on achieving the best possible fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = create_dls_silverbox(bs=16, win_sz=500, stp_sz=10)\n",
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55a4e4",
   "metadata": {},
   "source": [
    "## The Default: MAE Loss\n",
    "\n",
    "TSFast uses `nn.L1Loss()` (Mean Absolute Error) as the default training loss.\n",
    "MAE is more robust to outliers and measurement spikes than MSE (Mean Squared\n",
    "Error). MSE heavily penalizes large errors, which can cause the model to\n",
    "overfit to noisy data points. For system identification, where measurement\n",
    "noise and occasional spikes are common, MAE provides a more stable training\n",
    "signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101fde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_mae = RNNLearner(dls, rnn_type='lstm', loss_func=nn.L1Loss(), metrics=[fun_rmse])\n",
    "lrn_mae.fit_flat_cos(n_epoch=5, lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb9b20",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "While you train with a loss function, you evaluate with metrics. TSFast\n",
    "provides several standard metrics for system identification:\n",
    "\n",
    "- **`fun_rmse`** -- Root Mean Square Error. The standard reporting metric in\n",
    "  many fields. Penalizes large errors more than MAE because of the squaring.\n",
    "\n",
    "- **`nrmse`** -- RMSE normalized by the variance of each target variable.\n",
    "  This allows fair comparison across outputs with different scales. A value\n",
    "  of 0 means perfect prediction; a value of 1 means the model is no better\n",
    "  than predicting the mean.\n",
    "\n",
    "- **`nrmse_std`** -- RMSE normalized by the standard deviation of each target\n",
    "  variable. Similar to `nrmse` but uses std instead of variance for the\n",
    "  denominator.\n",
    "\n",
    "- **`mean_vaf`** -- Variance Accounted For, expressed as a percentage. Measures\n",
    "  what fraction of the target signal's variance is explained by the model.\n",
    "  100% means perfect prediction. This metric is widely used in the system\n",
    "  identification literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn = RNNLearner(dls, rnn_type='lstm', metrics=[fun_rmse, nrmse, nrmse_std, mean_vaf])\n",
    "lrn.fit_flat_cos(n_epoch=5, lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7502b",
   "metadata": {},
   "source": [
    "## SkipNLoss: Ignoring Transient Warmup\n",
    "\n",
    "RNNs start from a zero hidden state. During the first few timesteps, the\n",
    "hidden state is \"warming up\" and predictions are unreliable. `SkipNLoss`\n",
    "wraps any loss function to discard the first N timesteps from the loss\n",
    "computation. This prevents the optimizer from wasting effort on the\n",
    "unavoidable warmup transient.\n",
    "\n",
    "- **`n_skip=50`** -- skip the first 50 timesteps when computing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf25f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_loss = SkipNLoss(nn.L1Loss(), n_skip=50)\n",
    "lrn_skip = RNNLearner(dls, rnn_type='lstm', loss_func=skip_loss, metrics=[fun_rmse])\n",
    "lrn_skip.fit_flat_cos(n_epoch=5, lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a6076",
   "metadata": {},
   "source": [
    "## CutLoss: Evaluating a Window\n",
    "\n",
    "`CutLoss` slices the sequence to a specific range before computing the loss.\n",
    "This is useful when you only care about predictions in a particular part of\n",
    "the sequence.\n",
    "\n",
    "- **`l_cut=50`** -- trim 50 timesteps from the left (start of the sequence)\n",
    "- **`r_cut=450`** -- keep up to timestep 450 (trim from the right)\n",
    "\n",
    "This evaluates only timesteps 50 through 450 of each 500-step window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_loss = CutLoss(nn.L1Loss(), l_cut=50, r_cut=450)\n",
    "lrn_cut = RNNLearner(dls, rnn_type='lstm', loss_func=cut_loss, metrics=[fun_rmse])\n",
    "lrn_cut.fit_flat_cos(n_epoch=5, lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5761b65",
   "metadata": {},
   "source": [
    "## NormLoss: Scale-Invariant Training\n",
    "\n",
    "When your system has multiple outputs with very different magnitudes (e.g.,\n",
    "position in meters and velocity in m/s), the loss is dominated by the\n",
    "largest-scale output. `NormLoss` normalizes both predictions and targets\n",
    "before computing the loss, so all outputs contribute equally regardless of\n",
    "their physical scale.\n",
    "\n",
    "`NormLoss` takes the output normalization statistics from the DataLoaders\n",
    "(`dls.norm_stats.y`) and uses them to normalize both prediction and target\n",
    "tensors before passing them to the base loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34039e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_loss = NormLoss(nn.L1Loss(), dls.norm_stats.y)\n",
    "lrn_norm = RNNLearner(dls, rnn_type='lstm', loss_func=norm_loss, metrics=[fun_rmse])\n",
    "lrn_norm.fit_flat_cos(n_epoch=5, lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714734fe",
   "metadata": {},
   "source": [
    "## Weighted MAE\n",
    "\n",
    "`weighted_mae` applies log-spaced weights along the time axis, giving higher\n",
    "weight to earlier timesteps and lower weight to later ones. This is useful\n",
    "when early dynamics matter more than steady-state behavior, for example when\n",
    "modeling transient responses or step responses where the initial trajectory is\n",
    "most informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d12afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_wmae = RNNLearner(dls, rnn_type='lstm', loss_func=weighted_mae, metrics=[fun_rmse])\n",
    "lrn_wmae.fit_flat_cos(n_epoch=5, lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a967e1e",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **MAE (default)** is robust to outliers -- a good default for system\n",
    "  identification where measurement noise and spikes are common.\n",
    "- **`fun_rmse`**, **`nrmse`**, and **`mean_vaf`** are standard evaluation\n",
    "  metrics. `nrmse` enables fair comparison across different-scale outputs,\n",
    "  and `mean_vaf` reports the percentage of variance explained.\n",
    "- **`SkipNLoss`** excludes the RNN warmup transient from the loss, preventing\n",
    "  the optimizer from fitting the unavoidable zero-state startup.\n",
    "- **`CutLoss`** restricts the loss to a specific time window, useful when\n",
    "  only part of the sequence matters.\n",
    "- **`NormLoss`** enables scale-invariant training for multi-output systems by\n",
    "  computing the loss in normalized space.\n",
    "- **`weighted_mae`** emphasizes early timesteps, useful for transient-response\n",
    "  modeling.\n",
    "- Choose metrics that match your evaluation requirements -- different\n",
    "  applications call for different measures of model quality."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,scripts//py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
