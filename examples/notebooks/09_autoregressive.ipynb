{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326e1547",
   "metadata": {},
   "source": [
    "# Example 09: Autoregressive Models\n",
    "\n",
    "Autoregressive (AR) models feed their own predictions back as input, enabling\n",
    "free-running multi-step-ahead simulation. During training, they use \"teacher\n",
    "forcing\" (feeding true outputs), then switch to their own predictions at\n",
    "inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de693d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook builds on concepts from Examples 00-04. Make sure you are\n",
    "familiar with simulation (Example 02), prediction mode (Example 03), and\n",
    "model architectures (Example 04) before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c73cb8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a5186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfast.datasets.benchmark import create_dls_silverbox\n",
    "from tsfast.models.rnn import AR_RNNLearner, RNNLearner\n",
    "from tsfast.models.cnn import AR_TCNLearner\n",
    "from tsfast.learner.losses import fun_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd19d99",
   "metadata": {},
   "source": [
    "## What is Autoregressive Prediction?\n",
    "\n",
    "In standard simulation (Example 02), the model maps input u(t) to output y(t)\n",
    "in a single forward pass. In autoregressive mode:\n",
    "\n",
    "- **Training (teacher forcing)**: the model receives `[u(t), y_true(t-1)]` as\n",
    "  input. The `ARInitCB` callback concatenates the true target to the input\n",
    "  automatically.\n",
    "- **Inference (free-running)**: the model uses its own prediction\n",
    "  `[u(t), y_pred(t-1)]`. This tests whether the model is stable -- errors can\n",
    "  accumulate and cause divergence.\n",
    "\n",
    "AR models are more powerful for long-horizon prediction but require stronger\n",
    "regularization to stay stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b27b4",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f5de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = create_dls_silverbox(bs=16, win_sz=500, stp_sz=10)\n",
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842f099",
   "metadata": {},
   "source": [
    "## Standard Simulation Baseline\n",
    "\n",
    "Train a standard RNN for comparison. This model sees only the input u(t) and\n",
    "must predict y(t) without any output feedback. It serves as a baseline to\n",
    "highlight the difference autoregressive models make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_std = RNNLearner(dls, rnn_type='lstm', hidden_size=40, n_skip=50, metrics=[fun_rmse])\n",
    "lrn_std.fit_flat_cos(n_epoch=10, lr=3e-3)\n",
    "lrn_std.show_results(max_n=2)\n",
    "print(f\"Standard RNN: {lrn_std.validate()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d925f546",
   "metadata": {},
   "source": [
    "## Autoregressive RNN\n",
    "\n",
    "`AR_RNNLearner` wraps the model with autoregressive behavior and adds\n",
    "`TimeSeriesRegularizer` automatically. `alpha` and `beta` control activation\n",
    "and temporal regularization respectively -- AR models need these for stability.\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "- **`rnn_type='lstm'`**: use LSTM cells for the recurrent layer.\n",
    "- **`hidden_size=40`**: 40 hidden units in the LSTM.\n",
    "- **`alpha=1.0`**: penalty weight for large activations (AR regularization).\n",
    "- **`beta=1.0`**: penalty weight for abrupt activation changes between\n",
    "  timesteps (TAR regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0353df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_ar = AR_RNNLearner(\n",
    "    dls, rnn_type='lstm', hidden_size=40,\n",
    "    alpha=1.0, beta=1.0, metrics=[fun_rmse]\n",
    ")\n",
    "lrn_ar.fit_flat_cos(n_epoch=10, lr=3e-3)\n",
    "lrn_ar.show_results(max_n=2)\n",
    "print(f\"AR-RNN: {lrn_ar.validate()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d4c5fc",
   "metadata": {},
   "source": [
    "## Autoregressive TCN\n",
    "\n",
    "AR mode also works with temporal convolutional networks. `AR_TCNLearner`\n",
    "combines causal convolutions with autoregressive output feedback. The\n",
    "`hl_depth` parameter controls the number of TCN blocks (and therefore the\n",
    "receptive field, which is `2**hl_depth` timesteps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_ar_tcn = AR_TCNLearner(dls, hl_depth=4, metrics=[fun_rmse])\n",
    "lrn_ar_tcn.fit_flat_cos(n_epoch=10, lr=3e-3)\n",
    "lrn_ar_tcn.show_results(max_n=2)\n",
    "print(f\"AR-TCN: {lrn_ar_tcn.validate()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d852ae8",
   "metadata": {},
   "source": [
    "## Stability and Regularization\n",
    "\n",
    "AR models can diverge during free-running inference if prediction errors\n",
    "accumulate. Regularization helps:\n",
    "\n",
    "- **`alpha`** penalizes large activations, keeping the model in a\n",
    "  well-behaved region.\n",
    "- **`beta`** penalizes abrupt changes in predictions, encouraging smoothness.\n",
    "- Higher `alpha` and `beta` improve stability but may reduce accuracy on\n",
    "  easy regions.\n",
    "\n",
    "Train with stronger regularization to demonstrate the trade-off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_ar_strong = AR_RNNLearner(\n",
    "    dls, rnn_type='lstm', hidden_size=40,\n",
    "    alpha=3.0, beta=3.0, metrics=[fun_rmse]\n",
    ")\n",
    "lrn_ar_strong.fit_flat_cos(n_epoch=10, lr=3e-3)\n",
    "lrn_ar_strong.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225f9f8",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- AR models feed their own predictions back as input for multi-step-ahead\n",
    "  simulation.\n",
    "- Teacher forcing during training provides stable gradients; free-running at\n",
    "  inference tests stability.\n",
    "- `AR_RNNLearner` and `AR_TCNLearner` handle the autoregressive logic\n",
    "  automatically.\n",
    "- Regularization (`alpha`, `beta`) is essential to prevent error accumulation\n",
    "  and divergence.\n",
    "- Trade-off: stronger regularization leads to more stable but potentially\n",
    "  less accurate predictions."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,scripts//py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
