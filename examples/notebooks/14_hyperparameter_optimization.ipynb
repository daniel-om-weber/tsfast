{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0cc9806",
   "metadata": {},
   "source": [
    "# Example 14: Hyperparameter Optimization with Ray Tune\n",
    "\n",
    "Manually tuning hyperparameters -- learning rate, hidden size, model type --\n",
    "is tedious and error-prone. TSFast integrates with\n",
    "[Ray Tune](https://docs.ray.io/en/latest/tune/index.html) to automate the\n",
    "search. This example runs a small hyperparameter search to find the best\n",
    "model configuration for the Silverbox benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce92cec",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This example builds on concepts from:\n",
    "\n",
    "- **Example 00** -- data loading and model training basics\n",
    "- **Example 04** -- model architectures and `rnn_type`\n",
    "\n",
    "Make sure Ray Tune is installed:\n",
    "\n",
    "```bash\n",
    "uv sync --extra dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105af268",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56bd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfast.datasets.benchmark import create_dls_silverbox\n",
    "from tsfast.models.rnn import RNNLearner\n",
    "from tsfast.tune import HPOptimizer, log_uniform\n",
    "from tsfast.learner.losses import fun_rmse\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98ad57",
   "metadata": {},
   "source": [
    "## Why Hyperparameter Optimization?\n",
    "\n",
    "Model performance depends heavily on hyperparameters: learning rate, hidden\n",
    "size, architecture choice, and regularization strength. Finding the right\n",
    "combination by hand requires many experiments and careful record-keeping.\n",
    "\n",
    "Automated approaches help:\n",
    "\n",
    "- **Grid search** evaluates every combination -- thorough but expensive.\n",
    "- **Random search** samples randomly and is surprisingly effective in\n",
    "  high-dimensional spaces.\n",
    "- **Population-based training** evolves configurations during training,\n",
    "  combining exploration with exploitation.\n",
    "\n",
    "Ray Tune provides all of these strategies (and more) behind a unified API.\n",
    "TSFast's `HPOptimizer` wraps Ray Tune so you can search over model\n",
    "configurations with minimal boilerplate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f2792",
   "metadata": {},
   "source": [
    "## Prepare the DataLoaders\n",
    "\n",
    "We use the Silverbox benchmark with a small batch size and window size to\n",
    "keep the example lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8f807",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "dls = create_dls_silverbox(bs=16, win_sz=500, stp_sz=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b47b9",
   "metadata": {},
   "source": [
    "## Define a Learner Factory\n",
    "\n",
    "`HPOptimizer` needs a factory function that takes `(dls, config)` and returns\n",
    "a configured Learner. Ray Tune calls this function once per trial, each time\n",
    "with a different hyperparameter configuration sampled from the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learner(dls, config):\n",
    "    \"\"\"Create a configured RNNLearner from hyperparameter config.\"\"\"\n",
    "    return RNNLearner(\n",
    "        dls,\n",
    "        rnn_type=config[\"rnn_type\"],\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        n_skip=50,\n",
    "        metrics=[fun_rmse],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc68814",
   "metadata": {},
   "source": [
    "## Define the Search Space\n",
    "\n",
    "The search space is a plain dictionary where values are Ray Tune sampling\n",
    "primitives:\n",
    "\n",
    "- **`tune.choice`** -- samples uniformly from a list of discrete options.\n",
    "  Good for categorical parameters like architecture type or layer count.\n",
    "- **`log_uniform`** -- samples uniformly on a logarithmic scale. Ideal for\n",
    "  parameters that span orders of magnitude, such as learning rate.\n",
    "\n",
    "We start with a small search over two parameters: RNN cell type and hidden\n",
    "size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37302ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_config = {\n",
    "    \"rnn_type\": tune.choice([\"gru\", \"lstm\"]),\n",
    "    \"hidden_size\": tune.choice([32, 40]),\n",
    "    \"n_epoch\": 3,\n",
    "    \"lr\": 3e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf07af22",
   "metadata": {},
   "source": [
    "The config also contains fixed training parameters:\n",
    "\n",
    "- **`n_epoch=3`** -- each trial trains for 3 epochs (enough to compare\n",
    "  configurations, not enough for final training).\n",
    "- **`lr=3e-3`** -- fixed learning rate for all trials in this first search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7e95d",
   "metadata": {},
   "source": [
    "## Run the Optimization\n",
    "\n",
    "`HPOptimizer` takes the learner factory and the DataLoaders. Calling\n",
    "`optimize` launches the search: `num_samples=4` runs 4 independent trials,\n",
    "each with a different hyperparameter combination drawn from `search_config`.\n",
    "\n",
    "The default training function uses `fit_flat_cos` and reports training loss,\n",
    "validation loss, and metrics to Ray Tune after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = HPOptimizer(\n",
    "    create_lrn=create_learner,\n",
    "    dls=dls,\n",
    ")\n",
    "\n",
    "results = optimizer.optimize(\n",
    "    config=search_config,\n",
    "    num_samples=4,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717583e2",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "The `optimize` call returns a Ray Tune `ExperimentAnalysis` object stored in\n",
    "`optimizer.analysis`. You can query it for the best trial configuration,\n",
    "inspect per-trial results, or export data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = optimizer.analysis.get_best_config(metric=\"valid_loss\", mode=\"min\")\n",
    "print(\"Best config:\")\n",
    "for key in [\"rnn_type\", \"hidden_size\", \"lr\"]:\n",
    "    print(f\"  {key}: {best[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = optimizer.analysis.results_df\n",
    "print(\"\\nAll trial results:\")\n",
    "result_df[[\"config/rnn_type\", \"config/hidden_size\", \"valid_loss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a47c9a",
   "metadata": {},
   "source": [
    "## Using log_uniform for Learning Rate\n",
    "\n",
    "In the first search we fixed the learning rate. A more thorough search treats\n",
    "`lr` as a tunable parameter using `log_uniform`. This samples on a\n",
    "logarithmic scale between the given bounds -- appropriate because the\n",
    "difference between `1e-4` and `1e-3` matters more than between `1e-2` and\n",
    "`1.1e-2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ce887",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_config_v2 = {\n",
    "    \"rnn_type\": tune.choice([\"gru\", \"lstm\"]),\n",
    "    \"hidden_size\": tune.choice([32, 40]),\n",
    "    \"lr\": log_uniform(1e-4, 1e-2),\n",
    "    \"n_epoch\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daaea0b",
   "metadata": {},
   "source": [
    "When `lr` is a callable sampler in the config, the training function samples\n",
    "a fresh value for each trial. This overrides any fixed learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32500545",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_v2 = HPOptimizer(\n",
    "    create_lrn=create_learner,\n",
    "    dls=dls,\n",
    ")\n",
    "\n",
    "results_v2 = optimizer_v2.optimize(\n",
    "    config=search_config_v2,\n",
    "    num_samples=4,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a22851",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_v2 = optimizer_v2.analysis.get_best_config(metric=\"valid_loss\", mode=\"min\")\n",
    "print(\"Best config (with lr search):\")\n",
    "for key in [\"rnn_type\", \"hidden_size\", \"lr\"]:\n",
    "    print(f\"  {key}: {best_v2[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020a06a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **`HPOptimizer`** wraps Ray Tune for easy hyperparameter search with\n",
    "  TSFast. Pass a learner factory and DataLoaders, then call `optimize`.\n",
    "- **Learner factory** -- a function `(dls, config) -> Learner` that builds a\n",
    "  fresh model from the hyperparameter config each trial.\n",
    "- **`tune.choice`** for categorical parameters (architecture, layer count);\n",
    "  **`log_uniform`** for continuous parameters on a log scale (learning rate).\n",
    "- **Start small** -- few trials, few epochs -- to validate the pipeline\n",
    "  before scaling up.\n",
    "- **`optimizer.analysis`** gives access to the full Ray Tune\n",
    "  `ExperimentAnalysis` for querying best configs, exporting results, and\n",
    "  loading the best checkpoint."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,scripts//py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
