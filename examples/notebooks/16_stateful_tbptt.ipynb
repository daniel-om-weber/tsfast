{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d254e5",
   "metadata": {},
   "source": [
    "# Example 16: Stateful Models and TBPTT\n",
    "\n",
    "Standard backpropagation through time (BPTT) requires storing intermediate\n",
    "activations for every timestep in a sequence. For very long sequences this\n",
    "quickly exhausts GPU memory. Truncated Backpropagation Through Time (TBPTT)\n",
    "solves this by splitting long sequences into manageable sub-windows while\n",
    "carrying the RNN hidden state across them, enabling training on arbitrarily\n",
    "long sequences with bounded memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea0655",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This example builds on concepts from Examples 00-04. Make sure you have\n",
    "completed those first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9197035",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfast.datasets.benchmark import create_dls_silverbox\n",
    "from tsfast.models.rnn import RNNLearner\n",
    "from tsfast.learner.losses import fun_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6103e7",
   "metadata": {},
   "source": [
    "## The Memory Problem\n",
    "\n",
    "When training RNNs, backpropagation stores intermediate activations for every\n",
    "timestep. A sequence of length 10,000 requires roughly 100x more memory than\n",
    "a sequence of length 100. For very long sequences this exceeds GPU memory.\n",
    "\n",
    "TBPTT solves this by:\n",
    "\n",
    "1. **Splitting** the long sequence into sub-windows (e.g., 100 timesteps each)\n",
    "2. **Running** the forward pass on one sub-window at a time\n",
    "3. **Computing gradients** only within each sub-window (truncated)\n",
    "4. **Carrying the hidden state** from the end of one sub-window to the start\n",
    "   of the next\n",
    "\n",
    "This means the model still \"sees\" the full sequence through its hidden state,\n",
    "but memory usage is bounded by `sub_seq_len` rather than the full sequence\n",
    "length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0180211",
   "metadata": {},
   "source": [
    "## Standard Training (Baseline)\n",
    "\n",
    "First, train normally with a moderately large window to establish a reference\n",
    "point. The full 500-step sequence is backpropagated through in one pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c839f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_standard = create_dls_silverbox(bs=16, win_sz=500, stp_sz=10)\n",
    "\n",
    "lrn_standard = RNNLearner(dls_standard, rnn_type='lstm', hidden_size=40, metrics=[fun_rmse])\n",
    "lrn_standard.fit_flat_cos(n_epoch=10, lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7eaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_standard.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Standard: {lrn_standard.validate()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e14ac0",
   "metadata": {},
   "source": [
    "## TBPTT Training\n",
    "\n",
    "Now train with TBPTT by adding `sub_seq_len` to the DataLoader creation.\n",
    "This tells TSFast to split each window into sub-windows for truncated\n",
    "backpropagation.\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "- **`win_sz=500`** -- each training sample is 500 timesteps long (same as\n",
    "  the baseline above).\n",
    "- **`sub_seq_len=100`** -- split each 500-step window into 5 sub-windows of\n",
    "  100 timesteps each.\n",
    "\n",
    "Gradients flow through only 100 timesteps at a time (memory-efficient), but\n",
    "the hidden state spans all 500 timesteps within each window. The total\n",
    "sequence length `win_sz` must be divisible by `sub_seq_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88233dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_tbptt = create_dls_silverbox(bs=16, win_sz=500, stp_sz=10, sub_seq_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a1c95",
   "metadata": {},
   "source": [
    "## Stateful Model\n",
    "\n",
    "Create a stateful RNN that maintains hidden state across sub-windows.\n",
    "\n",
    "- **`stateful=True`** -- the RNN does **not** reset its hidden state between\n",
    "  forward passes. Instead, the state from the previous sub-window initializes\n",
    "  the next one. This is what allows information to flow across sub-window\n",
    "  boundaries.\n",
    "\n",
    "When `stateful=True` is set, `RNNLearner` automatically adds `TbpttResetCB`\n",
    "to the learner. This callback monitors the DataLoader and resets the hidden\n",
    "state whenever a new main window starts (i.e., at sequence boundaries).\n",
    "Without this reset, the hidden state from one training sample would bleed\n",
    "into the next, unrelated sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_tbptt = RNNLearner(\n",
    "    dls_tbptt, rnn_type='lstm', hidden_size=40,\n",
    "    stateful=True, metrics=[fun_rmse],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b205c23",
   "metadata": {},
   "source": [
    "## Train with TBPTT\n",
    "\n",
    "Training proceeds exactly like standard training. Under the hood, the\n",
    "DataLoader yields sub-windows in order, the stateful RNN carries hidden state\n",
    "across them, and `TbpttResetCB` resets state at sequence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abc54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_tbptt.fit_flat_cos(n_epoch=10, lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89784888",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_tbptt.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ec51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TBPTT: {lrn_tbptt.validate()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4ada1",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Compare the final validation metrics for both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Standard (full BPTT):  {lrn_standard.validate()}\")\n",
    "print(f\"Stateful (TBPTT):      {lrn_tbptt.validate()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e636c98d",
   "metadata": {},
   "source": [
    "TBPTT may have slightly different loss because gradients are truncated at\n",
    "sub-window boundaries. However, performance should be comparable. The key\n",
    "advantage is **memory efficiency** -- TBPTT can handle sequences that would\n",
    "cause out-of-memory errors with standard training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1ef8d",
   "metadata": {},
   "source": [
    "## When to Use TBPTT\n",
    "\n",
    "TBPTT is most useful when:\n",
    "\n",
    "- **Sequences are very long** (thousands of timesteps or more) and\n",
    "  full backpropagation would exhaust GPU memory.\n",
    "- **GPU memory is limited** and you need to keep memory usage bounded.\n",
    "- **The system has long-range dependencies** that benefit from a large\n",
    "  `win_sz`, but you cannot afford to backpropagate through the entire window.\n",
    "\n",
    "For short sequences (under ~1000 timesteps), standard training is simpler and\n",
    "usually sufficient. The overhead of managing sub-windows and stateful hidden\n",
    "state is not worth the complexity for sequences that already fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871fac61",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **TBPTT splits long sequences into sub-windows** with the `sub_seq_len`\n",
    "  parameter on `create_dls_silverbox` (or any `create_dls` variant).\n",
    "- **`stateful=True`** makes the RNN carry its hidden state across sub-windows\n",
    "  instead of resetting to zero each time.\n",
    "- **`TbpttResetCB`** resets hidden state at sequence boundaries so different\n",
    "  training samples do not bleed into each other. It is added automatically\n",
    "  when `stateful=True`.\n",
    "- **Gradients are truncated** to `sub_seq_len` timesteps, bounding memory\n",
    "  usage regardless of the full sequence length.\n",
    "- **Hidden state spans the full sequence**, preserving long-range information\n",
    "  even though gradients are truncated.\n",
    "- **Use TBPTT when sequences are too long** for standard backpropagation to\n",
    "  fit in GPU memory."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,scripts//py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
