{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fc2f8a",
   "metadata": {},
   "source": [
    "# Example 11: Benchmarking with IdentiBench\n",
    "\n",
    "IdentiBench provides standardized benchmarks for comparing system\n",
    "identification methods. This example shows how to run your TSFast models\n",
    "on IdentiBench benchmarks for fair, reproducible comparison with other\n",
    "methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f058d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- [Example 00: Your First Model](00_your_first_model.ipynb)\n",
    "- [Example 01: Understanding the Data Pipeline](01_data_pipeline.ipynb)\n",
    "- [Example 02: Simulation](02_simulation.ipynb)\n",
    "- [Example 04: Benchmark RNN](04_benchmark_rnn.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d8990",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d35058",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import identibench as idb\n",
    "\n",
    "from tsfast.datasets.benchmark import create_dls_from_spec\n",
    "from tsfast.models.rnn import RNNLearner\n",
    "from tsfast.inference import InferenceWrapper\n",
    "from tsfast.learner.losses import fun_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee4cc2",
   "metadata": {},
   "source": [
    "## What is IdentiBench?\n",
    "\n",
    "IdentiBench is a benchmarking framework that provides standardized\n",
    "datasets, evaluation protocols, and metrics for system identification.\n",
    "Each benchmark defines:\n",
    "\n",
    "- A **dataset** with specified train/validation/test splits\n",
    "- **Input and output column names** (e.g., voltage in, displacement out)\n",
    "- **Evaluation metrics** (typically NRMSE -- normalized root mean square\n",
    "  error)\n",
    "- A **standard API** that all methods must follow, ensuring fair\n",
    "  comparison\n",
    "\n",
    "The `workshop_benchmarks` dictionary contains the benchmarks used in the\n",
    "IdentiBench workshop -- a curated set covering different system types and\n",
    "difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f7239b",
   "metadata": {},
   "source": [
    "## The Build Model Function\n",
    "\n",
    "IdentiBench requires a `build_model` function that takes a\n",
    "`TrainingContext` and returns a callable model for evaluation. The context\n",
    "provides:\n",
    "\n",
    "- **`context.spec`** -- the benchmark specification (dataset path, column\n",
    "  names, window sizes, metric function)\n",
    "- **`context.hyperparameters`** -- your model's hyperparameters, passed\n",
    "  through from the benchmark runner\n",
    "\n",
    "The returned model must accept numpy arrays: `model(u_test, y_init)` for\n",
    "simulation benchmarks, where `u_test` is the full input signal and\n",
    "`y_init` is the initial output window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(context: idb.TrainingContext):\n",
    "    \"\"\"Build and train a TSFast model for an IdentiBench benchmark.\"\"\"\n",
    "    dls = create_dls_from_spec(context.spec)\n",
    "\n",
    "    lrn = RNNLearner(\n",
    "        dls,\n",
    "        rnn_type=context.hyperparameters.get('model_type', 'lstm'),\n",
    "        num_layers=context.hyperparameters.get('num_layers', 1),\n",
    "        hidden_size=context.hyperparameters.get('hidden_size', 100),\n",
    "        n_skip=context.spec.init_window,\n",
    "        metrics=[fun_rmse],\n",
    "    )\n",
    "\n",
    "    lrn.fit_flat_cos(n_epoch=10, lr=3e-3)\n",
    "    return InferenceWrapper(lrn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21070f",
   "metadata": {},
   "source": [
    "Key details:\n",
    "\n",
    "- **`create_dls_from_spec`** automatically extracts column names, window\n",
    "  sizes, and prediction settings from the benchmark spec. It also applies\n",
    "  benchmark-specific DataLoader defaults (e.g., batch size, step size)\n",
    "  from TSFast's `BENCHMARK_DL_KWARGS` table.\n",
    "- **`n_skip=context.spec.init_window`** uses the benchmark-defined\n",
    "  initialization window to skip the initial transient in the loss. This\n",
    "  matches IdentiBench's evaluation protocol, which discards the first\n",
    "  `init_window` timesteps.\n",
    "- **`InferenceWrapper`** wraps the trained learner into a numpy-in,\n",
    "  numpy-out callable that IdentiBench's evaluation harness can call\n",
    "  directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03fb42",
   "metadata": {},
   "source": [
    "## Configure and Run Benchmarks\n",
    "\n",
    "We define a hyperparameter dictionary and pass it along with the\n",
    "benchmarks to `idb.run_benchmarks`. The runner:\n",
    "\n",
    "1. Downloads each dataset (on first use)\n",
    "2. Calls `build_model` with the spec and hyperparameters\n",
    "3. Evaluates the returned model on the held-out test set\n",
    "4. Collects metrics into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'model_type': 'lstm',\n",
    "    'num_layers': 1,\n",
    "    'hidden_size': 100,\n",
    "}\n",
    "\n",
    "benchmarks = list(idb.workshop_benchmarks.values())\n",
    "results = idb.run_benchmarks(benchmarks, build_model, model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca10446",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "The results DataFrame shows the benchmark name, metric score, and\n",
    "training/test times for each benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa1d75",
   "metadata": {},
   "source": [
    "## Trying Different Configurations\n",
    "\n",
    "One of IdentiBench's strengths is making it easy to compare different\n",
    "model architectures on the same benchmarks. Here we try a GRU with 2\n",
    "layers instead of a single-layer LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_v2 = {\n",
    "    'model_type': 'gru',\n",
    "    'num_layers': 2,\n",
    "    'hidden_size': 100,\n",
    "}\n",
    "\n",
    "results_v2 = idb.run_benchmarks(benchmarks, build_model, model_config_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4a901",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **IdentiBench provides standardized, reproducible benchmarks** for fair\n",
    "  comparison across system identification methods.\n",
    "- The **`build_model` function** follows a simple API: receive a training\n",
    "  context, build and train a model, return an `InferenceWrapper`.\n",
    "- **`create_dls_from_spec`** handles dataset-specific configuration\n",
    "  automatically -- column names, window sizes, and prediction settings\n",
    "  are all extracted from the benchmark spec.\n",
    "- **Compare different architectures** (LSTM vs. GRU, depth, width) on\n",
    "  the same benchmarks with minimal code changes.\n",
    "- Results are **directly comparable** with other methods in the\n",
    "  IdentiBench ecosystem."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,scripts//py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
