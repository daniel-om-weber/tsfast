{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d637261",
   "metadata": {},
   "source": [
    "# Example 15: ONNX Export and Deployment\n",
    "\n",
    "Once you have trained a model, you often want to deploy it without requiring\n",
    "the full PyTorch/fastai stack. ONNX (Open Neural Network Exchange) is an open\n",
    "format that lets you run models with lightweight runtimes like ONNX Runtime.\n",
    "This example exports a trained model to ONNX and verifies numerical agreement\n",
    "with the PyTorch version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb13ff3",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook builds on Examples 00-02. In particular, you should be familiar\n",
    "with `InferenceWrapper` from Example 02 (numpy-based inference with a trained\n",
    "model). Make sure ONNX Runtime is installed:\n",
    "\n",
    "```bash\n",
    "uv sync --extra dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b3f33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tsfast.datasets.benchmark import create_dls_silverbox\n",
    "from tsfast.models.rnn import RNNLearner\n",
    "from tsfast.inference import InferenceWrapper\n",
    "from tsfast.inference.onnx import export_onnx, OnnxInferenceWrapper\n",
    "from tsfast.learner.losses import fun_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a64f2",
   "metadata": {},
   "source": [
    "## Train a Model\n",
    "\n",
    "We train a quick LSTM on the Silverbox benchmark so we have a model to export.\n",
    "See Example 00 for a detailed walkthrough of this step.\n",
    "\n",
    "- **`bs=16`** -- batch size\n",
    "- **`win_sz=500`** -- window length in timesteps\n",
    "- **`stp_sz=10`** -- stride between consecutive windows\n",
    "- **`hidden_size=40`** -- number of LSTM hidden units\n",
    "- **`metrics=[fun_rmse]`** -- track RMSE during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = create_dls_silverbox(bs=16, win_sz=500, stp_sz=10)\n",
    "lrn = RNNLearner(dls, rnn_type='lstm', hidden_size=40, metrics=[fun_rmse])\n",
    "lrn.fit_flat_cos(n_epoch=5, lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e078ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744d540f",
   "metadata": {},
   "source": [
    "## PyTorch Inference with InferenceWrapper\n",
    "\n",
    "As covered in Example 02, `InferenceWrapper` provides numpy-in / numpy-out\n",
    "inference using the PyTorch model. It handles normalization automatically:\n",
    "raw numpy arrays go in, raw numpy predictions come out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f53418",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_wrapper = InferenceWrapper(lrn)\n",
    "\n",
    "xb, yb = dls.valid.one_batch()\n",
    "np_input = xb.cpu().numpy()\n",
    "\n",
    "y_pytorch = pytorch_wrapper.inference(np_input)\n",
    "print(f\"Input shape:  {np_input.shape}\")\n",
    "print(f\"Output shape: {y_pytorch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c5b35",
   "metadata": {},
   "source": [
    "## Export to ONNX\n",
    "\n",
    "`export_onnx` converts the trained model to ONNX format. Normalization (input\n",
    "scaling and output denormalization) is **baked into** the ONNX graph -- the\n",
    "exported model accepts raw numpy inputs and produces raw outputs, just like\n",
    "`InferenceWrapper`.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- **`lrn`** -- the trained fastai Learner to export\n",
    "- **`path`** -- output file path (a `.onnx` suffix is added if missing)\n",
    "- **`opset_version=17`** (default) -- ONNX operator set version. Higher\n",
    "  versions support more operations; 17 is a safe default for most runtimes.\n",
    "- **`seq_len=None`** (default) -- override the sequence length for the dummy\n",
    "  input used during tracing. By default it uses the window size from the\n",
    "  DataLoaders. The exported model accepts any sequence length at runtime thanks\n",
    "  to dynamic axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = export_onnx(lrn, '/tmp/tsfast_model.onnx')\n",
    "print(f\"Exported to: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f733186",
   "metadata": {},
   "source": [
    "## Load and Run with ONNX Runtime\n",
    "\n",
    "`OnnxInferenceWrapper` loads the exported ONNX model and provides the same\n",
    "`.inference()` API as `InferenceWrapper`. Under the hood it uses ONNX Runtime,\n",
    "a lightweight inference engine that does **not** require PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03686fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_wrapper = OnnxInferenceWrapper(onnx_path)\n",
    "y_onnx = onnx_wrapper.inference(np_input)\n",
    "print(f\"ONNX output shape: {y_onnx.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbcf622",
   "metadata": {},
   "source": [
    "## Verify Numerical Agreement\n",
    "\n",
    "The PyTorch and ONNX outputs should be nearly identical. Small floating-point\n",
    "differences are expected due to different execution backends, but the maximum\n",
    "absolute difference should be well below 1e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_diff = np.max(np.abs(y_pytorch - y_onnx))\n",
    "mean_diff = np.mean(np.abs(y_pytorch - y_onnx))\n",
    "print(f\"Max absolute difference:  {max_diff:.2e}\")\n",
    "print(f\"Mean absolute difference: {mean_diff:.2e}\")\n",
    "assert max_diff < 1e-4, f\"Outputs differ by {max_diff}\"\n",
    "print(\"Outputs match within tolerance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a568d45",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "Autoregressive models (`AR_RNNLearner`, `AR_TCNLearner`) **cannot** be exported\n",
    "to ONNX. These models contain a sequential loop that feeds predictions back as\n",
    "input at each timestep, and ONNX does not support dynamic loops of this kind.\n",
    "If you try to export an autoregressive model, `export_onnx` will raise a\n",
    "`ValueError` with a clear message.\n",
    "\n",
    "For autoregressive models, use `InferenceWrapper` instead -- it runs the\n",
    "prediction loop in Python and works with any model type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a3d38",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **`export_onnx` converts trained models to ONNX format** with normalization\n",
    "  baked into the graph. The exported model accepts raw inputs and produces raw\n",
    "  outputs.\n",
    "- **`OnnxInferenceWrapper` provides the same numpy interface** as\n",
    "  `InferenceWrapper`, making it a drop-in replacement for deployment.\n",
    "- **ONNX Runtime is lightweight** -- deploy your models without installing\n",
    "  PyTorch or fastai.\n",
    "- **Always verify numerical agreement** between PyTorch and ONNX outputs to\n",
    "  catch export issues early.\n",
    "- **Autoregressive models cannot be exported** due to their sequential\n",
    "  prediction loop. Use `InferenceWrapper` for those models instead."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,scripts//py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
