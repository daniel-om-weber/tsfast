{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f08df01",
   "metadata": {},
   "source": [
    "# Example 13: Custom Data Pipelines with HDF5\n",
    "\n",
    "When your dataset doesn't fit the standard `create_dls` pattern -- multiple\n",
    "files, custom splits, weighted sampling -- you need a custom pipeline. This\n",
    "example shows how to compose tsfast's building blocks to create flexible data\n",
    "loading for any HDF5 dataset. You will learn how each primitive works on its\n",
    "own, and then combine them into a complete training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7d1f3",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- [Example 00: Your First Model](00_your_first_model.ipynb)\n",
    "- [Example 01: Understanding the Data Pipeline](01_data_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fdb83",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from fastai.data.block import DataBlock\n",
    "\n",
    "from tsfast.data.block import SequenceBlock\n",
    "from tsfast.data.core import (\n",
    "    CreateDict,\n",
    "    DfHDFCreateWindows,\n",
    "    TensorSequencesOutput,\n",
    "    ValidClmContains,\n",
    "    get_hdf_files,\n",
    ")\n",
    "from tsfast.data.loader import NBatches_Factory\n",
    "from tsfast.data.split import ParentSplitter\n",
    "from tsfast.datasets import create_dls\n",
    "from tsfast.learner.losses import fun_rmse\n",
    "from tsfast.models.rnn import RNNLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edeef5a",
   "metadata": {},
   "source": [
    "## Finding HDF5 Files\n",
    "\n",
    "The first step in any custom pipeline is discovering which files exist on\n",
    "disk. `get_hdf_files` recursively searches a directory for `.hdf5` and `.h5`\n",
    "files and returns them as a fastcore `L` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _root = Path(__file__).resolve().parent.parent\n",
    "except NameError:\n",
    "    _root = Path(\".\").resolve().parent\n",
    "\n",
    "data_path = _root / \"test_data\" / \"WienerHammerstein\"\n",
    "\n",
    "files = get_hdf_files(data_path)\n",
    "print(f\"Found {len(files)} HDF5 files:\")\n",
    "for f in files:\n",
    "    print(f\"  {f.parent.name}/{f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9bd98",
   "metadata": {},
   "source": [
    "## The Standard Approach\n",
    "\n",
    "Before building anything custom, let's see the standard `create_dls` call for\n",
    "reference. It handles file discovery, windowing, splitting, and normalization\n",
    "in a single function. All the primitives we explore below are composed\n",
    "internally by `create_dls`.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- **`u=['u']`** -- input signal column names in the HDF5 files.\n",
    "- **`y=['y']`** -- output signal column names the model learns to predict.\n",
    "- **`dataset`** -- path to a directory with `train/`, `valid/`, and `test/`\n",
    "  subdirectories containing HDF5 files.\n",
    "- **`win_sz=200`** -- window size in time steps. Each training sample is a\n",
    "  200-step slice.\n",
    "- **`stp_sz=50`** -- step size (stride) between consecutive windows.\n",
    "- **`bs=32`** -- batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_standard = create_dls(\n",
    "    u=['u'], y=['y'],\n",
    "    dataset=data_path,\n",
    "    win_sz=200, stp_sz=50,\n",
    "    bs=32,\n",
    ")\n",
    "dls_standard.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba9c0d",
   "metadata": {},
   "source": [
    "## Building a Custom Pipeline Step by Step\n",
    "\n",
    "Now let's rebuild the same pipeline manually to understand each component.\n",
    "This knowledge lets you customize any part of the pipeline when the standard\n",
    "approach doesn't fit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9997cf",
   "metadata": {},
   "source": [
    "### Step 1: Create Window Definitions\n",
    "\n",
    "`CreateDict` takes a list of file paths and produces a list of dictionaries,\n",
    "one per training sample. By passing `DfHDFCreateWindows`, each file is split\n",
    "into overlapping windows. By passing `ValidClmContains`, a `valid` column is\n",
    "added to mark validation and test files.\n",
    "\n",
    "Parameters for `DfHDFCreateWindows`:\n",
    "\n",
    "- **`win_sz=200`** -- the length of each window in time steps.\n",
    "- **`stp_sz=50`** -- the stride between consecutive windows.\n",
    "- **`clm='u'`** -- which HDF5 dataset to read for determining sequence\n",
    "  length. The function reads this dataset's shape to calculate how many\n",
    "  windows fit in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = CreateDict([\n",
    "    DfHDFCreateWindows(win_sz=200, stp_sz=50, clm='u'),\n",
    "    ValidClmContains(['valid', 'test']),\n",
    "])(files)\n",
    "\n",
    "print(f\"Total windows: {len(items)}\")\n",
    "print(f\"First item keys: {list(items[0].keys())}\")\n",
    "print(f\"First item: {items[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc1eda",
   "metadata": {},
   "source": [
    "Each item is a dictionary with:\n",
    "\n",
    "- **`path`** -- full file path to the HDF5 file.\n",
    "- **`l_slc`** / **`r_slc`** -- left and right slice boundaries defining the\n",
    "  window within the file.\n",
    "- **`valid`** -- `True` for files whose path contains `\"valid\"` or `\"test\"`,\n",
    "  used for train/validation splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeed2b8",
   "metadata": {},
   "source": [
    "### Step 2: Build a DataBlock\n",
    "\n",
    "The `DataBlock` ties together all the pieces: how to get items, how to split\n",
    "them, and how to extract tensors. `SequenceBlock.from_hdf` creates a\n",
    "`TransformBlock` that reads columns from HDF5 files and converts them to\n",
    "typed tensors.\n",
    "\n",
    "- **`blocks`** -- a tuple of `TransformBlock` objects defining input and\n",
    "  output types. The first block extracts input signals (`'u'`), the second\n",
    "  extracts output signals (`'y'`). Note that the output block uses\n",
    "  `TensorSequencesOutput` to distinguish it from inputs.\n",
    "- **`splitter`** -- determines how items are split into training and\n",
    "  validation sets. `ParentSplitter()` splits by parent directory name: files\n",
    "  in `train/` go to training, files in `valid/` go to validation.\n",
    "- **`get_items`** -- a callable that takes the source (file list) and returns\n",
    "  a list of items (dictionaries with window definitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(\n",
    "    blocks=(\n",
    "        SequenceBlock.from_hdf(['u']),\n",
    "        SequenceBlock.from_hdf(['y'], seq_cls=TensorSequencesOutput),\n",
    "    ),\n",
    "    splitter=ParentSplitter(),\n",
    "    get_items=CreateDict([DfHDFCreateWindows(win_sz=200, stp_sz=50, clm='u')]),\n",
    ")\n",
    "\n",
    "dls_custom = dblock.dataloaders(files, bs=32)\n",
    "dls_custom.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb2aab",
   "metadata": {},
   "source": [
    "## Fixed Batch Counts with NBatches_Factory\n",
    "\n",
    "When datasets have very different sizes, you may want a fixed number of\n",
    "batches per epoch regardless of how many windows exist. `NBatches_Factory`\n",
    "wraps a DataLoader class so it always yields exactly `n_batches` batches:\n",
    "it oversamples (with replacement) when there are fewer samples than needed,\n",
    "and undersamples when there are more.\n",
    "\n",
    "This is useful for:\n",
    "\n",
    "- **Consistent training time** across datasets of varying size.\n",
    "- **Curriculum learning** where you want to control exposure per epoch.\n",
    "- **Balancing** when combining datasets of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_nbatch = dblock.dataloaders(\n",
    "    files, bs=32,\n",
    "    dl_type=NBatches_Factory(dls_custom.train.__class__),\n",
    "    dl_kwargs=[{'n_batches': 100}, {'n_batches': None}],\n",
    ")\n",
    "print(f\"Training batches per epoch: {len(dls_nbatch.train)}\")\n",
    "print(f\"Validation batches per epoch: {len(dls_nbatch.valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240167e8",
   "metadata": {},
   "source": [
    "## Variable-Length Sequences with Padding\n",
    "\n",
    "When HDF5 files have different lengths and you want windows that cover the\n",
    "full file without overlap, the resulting sequences may vary in length across\n",
    "files. Setting `padding=True` on `SequenceBlock.from_hdf` adds a collation\n",
    "function that pads shorter sequences with zeros to match the longest sequence\n",
    "in each batch.\n",
    "\n",
    "This is useful when:\n",
    "\n",
    "- Files represent separate experiments of varying duration.\n",
    "- You want to process each file as a single window (no sub-windowing).\n",
    "- You need batch processing but your sequences have different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c15be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock_padded = DataBlock(\n",
    "    blocks=(\n",
    "        SequenceBlock.from_hdf(['u'], padding=True),\n",
    "        SequenceBlock.from_hdf(['y'], seq_cls=TensorSequencesOutput, padding=True),\n",
    "    ),\n",
    "    splitter=ParentSplitter(),\n",
    "    get_items=CreateDict([DfHDFCreateWindows(win_sz=200, stp_sz=200, clm='u')]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760277a",
   "metadata": {},
   "source": [
    "## Training with the Custom Pipeline\n",
    "\n",
    "Let's train an LSTM on the custom DataLoaders to verify everything works\n",
    "end-to-end. `RNNLearner` creates a recurrent neural network wrapped in a\n",
    "fastai Learner.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- **`dls_custom`** -- the DataLoaders we built manually above.\n",
    "- **`rnn_type='lstm'`** -- use Long Short-Term Memory cells.\n",
    "- **`hidden_size=100`** -- number of hidden units in the LSTM.\n",
    "- **`metrics=[fun_rmse]`** -- track root mean squared error during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74512b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn = RNNLearner(dls_custom, rnn_type='lstm', hidden_size=100, metrics=[fun_rmse])\n",
    "lrn.fit_flat_cos(n_epoch=5, lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn.show_results(max_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46b3ce",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **`get_hdf_files`** discovers HDF5 files recursively in a directory tree.\n",
    "- **`CreateDict`** + **`DfHDFCreateWindows`** creates window definitions from\n",
    "  file paths, where each item is a dictionary with file path and slice\n",
    "  boundaries.\n",
    "- **`ValidClmContains`** marks validation files by checking whether the file\n",
    "  path contains specific strings (e.g., `\"valid\"`, `\"test\"`).\n",
    "- **`DataBlock`** with **`SequenceBlock.from_hdf`** creates flexible\n",
    "  input/output pipelines that read directly from HDF5 files.\n",
    "- **`ParentSplitter`** splits data into train/validation sets based on parent\n",
    "  directory names (e.g., `train/` vs `valid/`).\n",
    "- **`NBatches_Factory`** ensures a fixed number of batches per epoch\n",
    "  regardless of dataset size.\n",
    "- **`padding=True`** handles variable-length sequences by zero-padding within\n",
    "  each batch.\n",
    "- The standard **`create_dls`** composes these same primitives internally --\n",
    "  understanding them lets you customize any part of the pipeline."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,scripts//py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
