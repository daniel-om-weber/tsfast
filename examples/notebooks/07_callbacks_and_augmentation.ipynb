{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4a46dd",
   "metadata": {},
   "source": [
    "# Example 07: Training Callbacks and Data Augmentation\n",
    "\n",
    "Callbacks let you customize every aspect of the training loop without\n",
    "modifying model code. Data augmentation transforms add noise or bias to\n",
    "training data, improving generalization. This example demonstrates both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d706fcc",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- [Example 00: Your First Model](00_your_first_model.ipynb)\n",
    "- [Example 01: Understanding the Data Pipeline](01_data_pipeline.ipynb)\n",
    "- [Example 02: Simulation](02_simulation.ipynb)\n",
    "- [Example 04: Benchmark RNN](04_benchmark_rnn.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e1302",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfast.datasets.benchmark import create_dls_silverbox\n",
    "from tsfast.models.rnn import RNNLearner\n",
    "from tsfast.learner.callbacks import (\n",
    "    TimeSeriesRegularizer, GradientClipping, VarySeqLen,\n",
    "    BatchLossFilter, CB_TruncateSequence,\n",
    ")\n",
    "from tsfast.data.transforms import SeqNoiseInjection, SeqBiasInjection\n",
    "from tsfast.learner.losses import fun_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb9f1f",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = create_dls_silverbox(bs=16, win_sz=500, stp_sz=10)\n",
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772882dc",
   "metadata": {},
   "source": [
    "## Data Augmentation Transforms\n",
    "\n",
    "Transforms modify training data on-the-fly. They only apply during training\n",
    "(not validation or test), so your evaluation metrics stay comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e900f0",
   "metadata": {},
   "source": [
    "### SeqNoiseInjection\n",
    "\n",
    "Adds Gaussian noise to input signals. `std` controls the noise magnitude and\n",
    "`p` is the probability of applying the transform per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fbf4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_noisy = create_dls_silverbox(bs=16, win_sz=500, stp_sz=10)\n",
    "dls_noisy.train.after_batch.add(SeqNoiseInjection(std=0.05, p=1.0))\n",
    "dls_noisy.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86c5c8",
   "metadata": {},
   "source": [
    "Compare this to the clean batch above -- you should see slight noise on the\n",
    "input signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659864bf",
   "metadata": {},
   "source": [
    "### SeqBiasInjection\n",
    "\n",
    "Adds a constant offset per signal per sample. This simulates sensor drift or\n",
    "calibration errors, making the model more robust to such shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff72551",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_biased = create_dls_silverbox(bs=16, win_sz=500, stp_sz=10)\n",
    "dls_biased.train.after_batch.add(SeqBiasInjection(std=0.1, p=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4739fcb",
   "metadata": {},
   "source": [
    "### Training with Augmentation\n",
    "\n",
    "Train two models -- one with augmentation, one without -- to see the effect\n",
    "on validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_base = RNNLearner(dls, rnn_type='lstm', metrics=[fun_rmse])\n",
    "lrn_base.fit_flat_cos(n_epoch=5, lr=3e-3)\n",
    "print(f\"Without augmentation: {lrn_base.validate()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ee3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_aug = RNNLearner(dls_noisy, rnn_type='lstm', metrics=[fun_rmse])\n",
    "lrn_aug.fit_flat_cos(n_epoch=5, lr=3e-3)\n",
    "print(f\"With noise augmentation: {lrn_aug.validate()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab8470",
   "metadata": {},
   "source": [
    "## TimeSeriesRegularizer\n",
    "\n",
    "Adds two regularization terms to the loss:\n",
    "\n",
    "- **`alpha`**: L2 penalty on RNN activations -- prevents activations from\n",
    "  growing too large.\n",
    "- **`beta`**: L2 penalty on temporal differences of activations -- encourages\n",
    "  smooth predictions over time.\n",
    "\n",
    "`modules` specifies which model components to regularize (typically the RNN\n",
    "layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ed07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_reg = RNNLearner(dls, rnn_type='lstm', metrics=[fun_rmse])\n",
    "lrn_reg.fit_flat_cos(n_epoch=5, lr=3e-3, cbs=[\n",
    "    TimeSeriesRegularizer(alpha=2.0, beta=1.0)\n",
    "])\n",
    "lrn_reg.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7512b921",
   "metadata": {},
   "source": [
    "## GradientClipping\n",
    "\n",
    "Clips the gradient norm during backpropagation. This prevents exploding\n",
    "gradients, which are common with RNNs on long sequences. `clip_val` is the\n",
    "maximum allowed gradient norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_clip = RNNLearner(dls, rnn_type='lstm', metrics=[fun_rmse])\n",
    "lrn_clip.fit_flat_cos(n_epoch=5, lr=3e-3, cbs=[GradientClipping(clip_val=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a259ec2",
   "metadata": {},
   "source": [
    "## VarySeqLen\n",
    "\n",
    "Randomly truncates sequences to different lengths each batch. This acts as\n",
    "data augmentation by preventing the model from overfitting to a fixed window\n",
    "size. `min_len` sets the minimum allowed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54509e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_vary = RNNLearner(dls, rnn_type='lstm', metrics=[fun_rmse])\n",
    "lrn_vary.fit_flat_cos(n_epoch=5, lr=3e-3, cbs=[VarySeqLen(min_len=100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4376fa7",
   "metadata": {},
   "source": [
    "## BatchLossFilter\n",
    "\n",
    "Keeps only the hardest batches (those with the highest loss) for gradient\n",
    "updates. `loss_perc=0.5` means only the top 50% of samples by loss\n",
    "contribute to learning -- a form of curriculum learning that focuses on the\n",
    "most informative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a920d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_filter = RNNLearner(dls, rnn_type='lstm', metrics=[fun_rmse])\n",
    "lrn_filter.fit_flat_cos(n_epoch=5, lr=3e-3, cbs=[BatchLossFilter(loss_perc=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75554d3",
   "metadata": {},
   "source": [
    "## CB_TruncateSequence\n",
    "\n",
    "Progressively increases sequence length during training. Starts with short\n",
    "sequences (easier for the model) and gradually increases to full length.\n",
    "This is a form of curriculum learning that helps the model learn short-term\n",
    "dynamics first before tackling longer dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a46ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_trunc = RNNLearner(dls, rnn_type='lstm', metrics=[fun_rmse])\n",
    "lrn_trunc.fit_flat_cos(n_epoch=10, lr=3e-3, cbs=[CB_TruncateSequence(truncate_length=100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cee049",
   "metadata": {},
   "source": [
    "## Combining Callbacks\n",
    "\n",
    "Multiple callbacks can be combined. They execute in order during each\n",
    "training step, so you can layer regularization, gradient control, and\n",
    "curriculum strategies together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58916940",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn_combined = RNNLearner(dls, rnn_type='lstm', metrics=[fun_rmse])\n",
    "lrn_combined.fit_flat_cos(n_epoch=10, lr=3e-3, cbs=[\n",
    "    TimeSeriesRegularizer(alpha=2.0, beta=1.0),\n",
    "    GradientClipping(clip_val=10),\n",
    "])\n",
    "lrn_combined.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e552dc",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **`SeqNoiseInjection`** and **`SeqBiasInjection`** augment training data\n",
    "  for better generalization.\n",
    "- **`TimeSeriesRegularizer`** smooths predictions with activation and\n",
    "  temporal penalties.\n",
    "- **`GradientClipping`** prevents exploding gradients on long sequences.\n",
    "- **`VarySeqLen`** acts as augmentation by varying sequence length each\n",
    "  batch.\n",
    "- **`BatchLossFilter`** focuses learning on the hardest examples.\n",
    "- **`CB_TruncateSequence`** implements curriculum learning with progressive\n",
    "  sequence length.\n",
    "- Callbacks compose -- combine multiple for best results."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,scripts//py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
