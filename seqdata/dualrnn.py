# AUTOGENERATED! DO NOT EDIT! File to edit: 11_dualrnn.ipynb (unless otherwise specified).

__all__ = ['ProDiagTrainer', 'DualRNN', 'DualCRNN']

# Cell
from .core import *
from .models.core import *
from .learner import *
from .dataloaders import *
from fastai.basics import *
from fastai.callback.progress import *
from fastai.callback.schedule import *

# Cell
class ProDiagTrainer(Callback):
    "`Callback` that regroups lr adjustment to seq_len, AR and TAR."
    def __init__(self, alpha=1e6,beta=1,p_own_state=0):
        store_attr('alpha,beta,p_own_state')
        self.main_init_prop = None

    def _has_main_init(self):
        return hasattr(self.learn.model,'main_init_prop')

    def before_fit(self):
        if self._has_main_init():
            self.main_init_prop=self.learn.model.main_init_prop

    def before_batch(self):
        if not self.training or self.p_own_state == 0: return
        main_init_prop = random.random()< self.p_own_state
        self.learn.model.main_init_prop = main_init_prop

    def after_pred(self):
        p,self.pred_diag,self.est_hidden,self.pred_hidden=self.pred
        self.learn.pred = p

    def after_loss(self):
        if not self.training: return
        self.learn.loss_grad += self.beta*self.learn.loss_func(self.pred_diag,*self.yb)

        hidden_loss = ((self.est_hidden-self.pred_hidden)/
                       (self.est_hidden.norm()+self.pred_hidden.norm())).pow(2).mean()
        self.learn.loss_grad += self.alpha * hidden_loss

    def before_validate(self):
        '''Set Dual RNN to reuse the prediction state after each mini batch on validation'''
        if self._has_main_init():
            self.learn.model.main_init_prop = True

    def after_validate(self):
        '''Reset Dual RNN to training state propagation behaviour'''
        if self._has_main_init():
            self.learn.model.main_init_prop=self.main_init_prop


    def after_fit(self):
        reset_model_state(self.learn.model)

# Cell
class DualRNN(nn.Module):

    @delegates(RNN, keep=True)
    def __init__(self,main_input_size,co_input_size,output_size,init_sz=100,hidden_size=100,
                 rnn_layer=1,linear_layer = 1,main_init_est = True,main_init_prop = True,**kwargs):
        super().__init__()
        store_attr('main_input_size,co_input_size,main_init_est,main_init_prop,init_sz')

        rnn_kwargs = dict(hidden_size=hidden_size,num_layers=rnn_layer,stateful=True,ret_full_hidden=True)
        rnn_kwargs = dict(rnn_kwargs, **kwargs)

        self.co_rnn = RNN(co_input_size,**rnn_kwargs)
        self.main_rnn = RNN(main_input_size,**rnn_kwargs)

#         self.co_estimator = SeqLinear(hidden_size,output_size,hidden_layer=linear_layer)
        self.main_estimator = SeqLinear(hidden_size,output_size,hidden_layer=linear_layer)

    def forward(self, x,init_state = None):
        bs = x.shape[0]
        if init_state is None:
            init_state = self.main_rnn._get_hidden(bs) if self.main_init_prop else self.co_rnn._get_hidden(bs)


        x_co = x[...,:self.co_input_size]
        x_main = x[...,:self.main_input_size]

        #RNN Layer
        if init_state is None:
#             import pdb; pdb.set_trace()
            if self.main_init_est:
                out_init,h_init = self.main_rnn(x_main[:,:self.init_sz])
                out_main,_ = self.main_rnn(x_main[:,self.init_sz:],h_init)
                out_co,_ = self.co_rnn(x_co[:,self.init_sz:],h_init)
                out_main=torch.cat([out_init,out_main],2)
                out_co=torch.cat([out_init,out_co],2)
            else:
                out_init,h_init = self.co_rnn(x_co[:,:self.init_sz])
                out_co,_ = self.co_rnn(x_co[:,self.init_sz:],h_init)
                out_main,_ = self.main_rnn(x_main[:,self.init_sz:],h_init)
                out_main=torch.cat([out_init,out_main],2)
                out_co=torch.cat([out_init,out_co],2)
        else:
            out_co,_ = self.co_rnn(x_co,init_state)
            out_main,_ = self.main_rnn(x_main,init_state)


#         import pdb; pdb.set_trace()

        #Shared Linear Layer
        est_co = self.main_estimator(out_co[-1])
        est_main = self.main_estimator(out_main[-1])

#         import pdb; pdb.set_trace()
        return est_main,est_co, out_co,out_main

# Cell
class DualCRNN(nn.Module):

    @delegates(RNN, keep=True)
    def __init__(self,main_input_size,co_input_size,output_size,init_sz=100,tcn_hidden_size=100,tcn_layer=8,rnn_hidden_size=100,
                 rnn_layer=1,linear_layer = 1,main_init_est = True,main_init_prop = True,**kwargs):
        super().__init__()
        store_attr('main_input_size,co_input_size,main_init_est,main_init_prop,init_sz')

        rnn_kwargs = dict(hidden_size=rnn_hidden_size,num_layers=rnn_layer,stateful=True,ret_full_hidden=True)
        rnn_kwargs = dict(rnn_kwargs, **kwargs)

        self.co_tcn = TCN(co_input_size,rnn_hidden_size,tcn_layer,tcn_hidden_size,stateful=True)
        self.main_tcn = TCN(main_input_size,rnn_hidden_size,tcn_layer,tcn_hidden_size,stateful=True)

        self.co_rnn = RNN(rnn_hidden_size,**rnn_kwargs)
        self.main_rnn = RNN(rnn_hidden_size,**rnn_kwargs)

#         self.co_estimator = SeqLinear(rnn_hidden_size,output_size,hidden_layer=linear_layer)
        self.main_estimator = SeqLinear(rnn_hidden_size,output_size,hidden_layer=linear_layer)

    def forward(self, x,init_state = None):
        bs = x.shape[0]
        if init_state is None:
            init_state = self.main_rnn._get_hidden(bs) if self.main_init_prop else self.co_rnn._get_hidden(bs)


        x_co = x[...,:self.co_input_size]
        x_main = x[...,:self.main_input_size]

        #TCN Layer
        x_co = self.co_tcn(x_co)
        x_main = self.main_tcn(x_main)


        #RNN Layer
        if init_state is None:
#             import pdb; pdb.set_trace()
            if self.main_init_est:
                out_init,h_init = self.main_rnn(x_main[:,:self.init_sz])
                out_main,_ = self.main_rnn(x_main[:,self.init_sz:],h_init)
                out_co,_ = self.co_rnn(x_co[:,self.init_sz:],h_init)
                out_main=torch.cat([out_init,out_main],2)
                out_co=torch.cat([out_init,out_co],2)
            else:
                out_init,h_init = self.co_rnn(x_co[:,:self.init_sz])
                out_co,_ = self.co_rnn(x_co[:,self.init_sz:],h_init)
                out_main,_ = self.main_rnn(x_main[:,self.init_sz:],h_init)
                out_main=torch.cat([out_init,out_main],2)
                out_co=torch.cat([out_init,out_co],2)
        else:
            out_co,_ = self.co_rnn(x_co,init_state)
            out_main,_ = self.main_rnn(x_main,init_state)


#         import pdb; pdb.set_trace()

        #Shared Linear Layer
        est_co = self.main_estimator(out_co[-1])
        est_main = self.main_estimator(out_main[-1])

#         import pdb; pdb.set_trace()
        return est_main,est_co, out_co,out_main

    def get_main_crnn(self):
        crnn_model = CRNN(1,1)
        crnn_model.cnn = self.main_tcn

        simple_rnn_model = SimpleRNN(1,1)
        simple_rnn_model.rnn = self.main_rnn
        simple_rnn_model.final = self.main_estimator
        crnn_model.rnn = simple_rnn_model
        return crnn_model


    def get_co_crnn(self):
        crnn_model = CRNN(1,1)
        crnn_model.cnn = self.co_tcn

        simple_rnn_model = SimpleRNN(1,1)
        simple_rnn_model.rnn = self.co_rnn
        simple_rnn_model.final = self.main_estimator
        crnn_model.rnn = simple_rnn_model
        return crnn_model