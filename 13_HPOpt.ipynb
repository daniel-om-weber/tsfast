{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp hpopt\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization Module\n",
    "> Pytorch Models for Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from seqdata.core import *\n",
    "from seqdata.models.core import *\n",
    "from seqdata.learner import *\n",
    "from fastai.basics import *\n",
    "from fastai.callback.schedule import *\n",
    "from fastai.callback.rnn import *\n",
    "from fastai.callback.tracker import *\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers import *\n",
    "from ray.tune.trial import ExportFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_paths = '/mnt/data/Systemidentification/WienerHammerstein/'\n",
    "hdf_files = L([f for f in get_hdf_files(f_paths) if 'test' not in str(f)])\n",
    "tfm_src = CreateDict([DfHDFCreateWindows(win_sz=400,stp_sz=100,clm='u')])\n",
    "dls = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n",
    "                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o)))).dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a log uniform distibution for variables with vast value ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def log_uniform(min_bound, max_bound, base=10):\n",
    "    '''uniform sampling in an exponential range'''\n",
    "    logmin = np.log(min_bound) / np.log(base)\n",
    "    logmax = np.log(max_bound) / np.log(base)\n",
    "    def _sample():\n",
    "        return base**(np.random.uniform(logmin, logmax))\n",
    "    return _sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.060701132813442e-06,\n",
       " 0.0004673308374999887,\n",
       " 3.9048465726471903e-05,\n",
       " 3.509406157988824e-08,\n",
       " 2.9457054267975667e-06]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[log_uniform(1e-8, 1e-2)() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LearnerTrainable(tune.Trainable):\n",
    "\n",
    "    def setup(self, config):\n",
    "        self.create_lrn = ray.get(config['create_lrn'])\n",
    "        self.dls = ray.get(config['dls'])\n",
    "\n",
    "        self.lrn = self.create_lrn(self.dls,config)\n",
    "\n",
    "    def step(self):\n",
    "        with self.lrn.no_bar(): self.lrn.fit(1)\n",
    "        train_loss,valid_loss,rmse = self.lrn.recorder.values[-1]\n",
    "        result = {'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'mean_loss': rmse}\n",
    "        return result\n",
    "\n",
    "    def save_checkpoint(self, tmp_checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
    "        torch.save(self.lrn.model.state_dict(), checkpoint_path)\n",
    "        return tmp_checkpoint_dir\n",
    "\n",
    "    def load_checkpoint(self, tmp_checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
    "        self.lrn.model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    def _export_model(self, export_formats, export_dir):\n",
    "        if export_formats == [ExportFormat.MODEL]:\n",
    "            path = os.path.join(export_dir, \"exported_model\")\n",
    "            torch.save(self.lrn.model.state_dict(), path)\n",
    "            return {ExportFormat.MODEL: path}\n",
    "        else:\n",
    "            raise ValueError(\"unexpected formats: \" + str(export_formats))\n",
    "\n",
    "    # the learner class will be recreated with every perturbation, saving the model\n",
    "    # that way the new hyperparameter will be applied\n",
    "    def reset_config(self, new_config):\n",
    "        self.lrn = self.create_lrn(self.dls,new_config)\n",
    "        self.config = new_config\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.callback.tracker import SaveModelCallback \n",
    "class CBRaySaveModel(SaveModelCallback):\n",
    "    \"A `TrackerCallback` that saves the model's best during training in a tune checkpoint directory\"\n",
    "    \n",
    "    def _save(self, name):\n",
    "        if not hasattr(self,'idx_checkpoint'): self.idx_checkpoint = 0\n",
    "        with tune.checkpoint_dir(step=self.idx_checkpoint) as checkpoint_dir:\n",
    "            file = os.path.join(checkpoint_dir,name+'.pth')\n",
    "            print(file)\n",
    "            save_model(file, self.learn.model,opt=None)\n",
    "            self.last_saved_path = file\n",
    "            self.idx_checkpoint += 1\n",
    "            \n",
    "    #final checkpoint\n",
    "    def after_fit(self, **kwargs):\n",
    "        self._save(f'{self.fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def learner_optimize(config, checkpoint_dir=None):\n",
    "        create_lrn = ray.get(config['create_lrn'])\n",
    "        dls = ray.get(config['dls'])\n",
    "        \n",
    "        #Scheduling Parameters for training the Model\n",
    "        lrn_kwargs = {'n_epoch':100,'pct_start':0.5}\n",
    "        for attr in ['n_epoch','pct_start']:\n",
    "            if attr in config: lrn_kwargs[attr] = config[attr]\n",
    "\n",
    "        lrn = create_lrn(dls,config)\n",
    "        \n",
    "        # load checkpoint data if provided\n",
    "        if checkpoint_dir:\n",
    "            lrn.model.load_state_dict(torch.load(tmp_checkpoint_dir))\n",
    "        \n",
    "        lrn.lr = config['lr'] if 'lr' in config else 3e-3\n",
    "        lrn.add_cb(CBRayReporter() if 'reporter' not in config else ray.get(config['reporter'])())\n",
    "        lrn.add_cb(CBRaySaveModel())\n",
    "        with lrn.no_bar(): \n",
    "            ray.get(config['fit_method'])(lrn,**lrn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutation config dictionary consists of functions that sample from a distribution. In order to retrieve a dictionary with one realisation we need the function sample_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sample_config(config):\n",
    "    ret_conf = config.copy()\n",
    "    for k in ret_conf:\n",
    "        ret_conf[k]=ret_conf[k]()\n",
    "    return ret_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CBRayReporter(Callback):\n",
    "    \"`Callback` reports progress after every epoch to the ray tune logger\"\n",
    "\n",
    "    def after_epoch(self):\n",
    "        train_loss,valid_loss,rmse = self.learn.recorder.values[-1]\n",
    "        tune.report(train_loss=train_loss,\n",
    "                        valid_loss=valid_loss,\n",
    "                        mean_loss=rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HPOptimizer():\n",
    "    def __init__(self,create_lrn,dls):\n",
    "        self.create_lrn = create_lrn\n",
    "        self.dls = dls\n",
    "        self.analysis = None\n",
    "    \n",
    "    @delegates(ray.init)\n",
    "    def start_ray(self,**kwargs):\n",
    "        ray.shutdown()\n",
    "        ray.init(**kwargs)\n",
    "        \n",
    "    def stop_ray(self):\n",
    "        ray.shutdown()\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    @delegates(tune.run, keep=True)\n",
    "    def optimize(self,config,optimize_func=learner_optimize,resources_per_trial={\"gpu\": 1.0},verbose=1,**kwargs):\n",
    "        config['create_lrn'] = ray.put(self.create_lrn)\n",
    "        #dls are large objects, letting ray handle the copying process makes it much faster\n",
    "        config['dls'] = ray.put(self.dls) \n",
    "        if 'fit_method' not in config: config['fit_method'] = ray.put(Learner.fit_flat_cos)\n",
    "\n",
    "        self.analysis = tune.run(\n",
    "            optimize_func,\n",
    "            config=config,\n",
    "            resources_per_trial=resources_per_trial,\n",
    "            verbose=verbose,\n",
    "            **kwargs)\n",
    "        return self.analysis\n",
    "        \n",
    "    @delegates(tune.run, keep=True)\n",
    "    def optimize_pbt(self,opt_name,num_samples,config,mut_conf,perturbation_interval=2,\n",
    "                 stop={\"training_iteration\": 40 },\n",
    "                 resources_per_trial={\"gpu\": 1 },\n",
    "                 resample_probability=0.25,\n",
    "                 quantile_fraction=0.25,\n",
    "                 **kwargs):\n",
    "        self.mut_conf = mut_conf\n",
    "        \n",
    "        config['create_lrn'] = ray.put(self.create_lrn)\n",
    "        #dls are large objects, letting ray handle the copying process makes it much faster\n",
    "        config['dls'] = ray.put(self.dls) \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"mean_loss\",\n",
    "        mode=\"min\",\n",
    "        perturbation_interval=perturbation_interval,\n",
    "        resample_probability=resample_probability,\n",
    "        quantile_fraction=quantile_fraction,\n",
    "        hyperparam_mutations=mut_conf)\n",
    "        \n",
    "        self.analysis = tune.run(\n",
    "            LearnerTrainable,\n",
    "            name=opt_name,\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors=True,\n",
    "            verbose=1,\n",
    "            stop=stop,\n",
    "            checkpoint_score_attr=\"mean_loss\",\n",
    "            num_samples=num_samples,\n",
    "            resources_per_trial=resources_per_trial,\n",
    "            config=config,\n",
    "            **kwargs)\n",
    "        return self.analysis\n",
    "    \n",
    "    def best_model(self):\n",
    "        if self.analysis is None: raise Exception\n",
    "        model = self.create_lrn(self.dls,sample_config(self.mut_conf)).model\n",
    "        f_path = self.analysis.get_best_trial('mean_loss',mode='min').checkpoint.value\n",
    "        model.load_state_dict(torch.load(f_path))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Population Based Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrn(dls,config):\n",
    "    lr = config['lr']\n",
    "    alpha = config['alpha']\n",
    "    beta = config['beta']\n",
    "    weight_p = config['weight_p']\n",
    "    \n",
    "    lrn = RNNLearner(dls,cbs=[TimeSeriesRegularizer(alpha,beta)],weight_p=weight_p)\n",
    "    lrn.lr = lr\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/125.8 GiB<br>PopulationBasedTraining: 2 checkpoints, 1 perturbs<br>Resources requested: 0/64 CPUs, 0.0/2 GPUs, 0.0/72.27 GiB heap, 0.0/24.12 GiB objects (0/1.0 accelerator_type:RTX)<br>Result logdir: /mnt/data/ray_results/pbt_test<br>Number of trials: 4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    alpha</th><th style=\"text-align: right;\">       beta</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  weight_p</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  valid_loss</th><th style=\"text-align: right;\">  neg_mean_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LearnerTrainable_a19b4_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.839598 </td><td style=\"text-align: right;\">0.000290224</td><td style=\"text-align: right;\">0.00727677 </td><td style=\"text-align: right;\">  0.314959</td><td style=\"text-align: right;\">0.239955</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         8.7491 </td><td style=\"text-align: right;\">   0.0546319</td><td style=\"text-align: right;\">   0.0576309</td><td style=\"text-align: right;\">      -0.239955</td></tr>\n",
       "<tr><td>LearnerTrainable_a19b4_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0023256</td><td style=\"text-align: right;\">0.0036981  </td><td style=\"text-align: right;\">0.000606961</td><td style=\"text-align: right;\">  0.433086</td><td style=\"text-align: right;\">0.24792 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         8.55515</td><td style=\"text-align: right;\">   0.0567358</td><td style=\"text-align: right;\">   0.0615294</td><td style=\"text-align: right;\">      -0.24792 </td></tr>\n",
       "<tr><td>LearnerTrainable_a19b4_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.032015 </td><td style=\"text-align: right;\">1.4956e-05 </td><td style=\"text-align: right;\">0.00285485 </td><td style=\"text-align: right;\">  0.340243</td><td style=\"text-align: right;\">0.239478</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         8.31972</td><td style=\"text-align: right;\">   0.0534964</td><td style=\"text-align: right;\">   0.0573912</td><td style=\"text-align: right;\">      -0.239478</td></tr>\n",
       "<tr><td>LearnerTrainable_a19b4_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.699665 </td><td style=\"text-align: right;\">0.154226   </td><td style=\"text-align: right;\">0.00909596 </td><td style=\"text-align: right;\">  0.393699</td><td style=\"text-align: right;\">0.219238</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         8.56191</td><td style=\"text-align: right;\">   0.0491459</td><td style=\"text-align: right;\">   0.0481051</td><td style=\"text-align: right;\">      -0.219238</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f71b8e11810>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\n",
    "            \"lr\": tune.loguniform(1e-2, 1e-4),\n",
    "            \"alpha\": tune.loguniform(1e-5, 10),\n",
    "            \"beta\": tune.loguniform(1e-5, 10),\n",
    "            \"weight_p\": tune.uniform(0, 0.5)}\n",
    "mut_conf = {# distribution for resampling\n",
    "            \"lr\": log_uniform(1e-8, 1e-2),\n",
    "            \"alpha\": log_uniform(1e-5, 10),\n",
    "            \"beta\": log_uniform(1e-5, 10),\n",
    "            \"weight_p\": lambda: np.random.uniform(0, 0.5)}\n",
    "\n",
    "hp_opt = HPOptimizer(create_lrn,dls)\n",
    "hp_opt.start_ray()\n",
    "hp_opt.optimize_pbt('pbt_test',4,config,mut_conf,perturbation_interval=1,\n",
    "                 stop={\"training_iteration\": 3 },\n",
    "                 resources_per_trial={\"gpu\": 0.5},\n",
    "                 local_dir='/mnt/data/ray_results')#no cpu count is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ray._raylet.ObjectRef' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ray._raylet.ObjectRef' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-db950a0b59dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhp_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-7cc9982f055d>\u001b[0m in \u001b[0;36mbest_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_lrn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmut_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_open_buffer_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_open_buffer_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected 'r' or 'w' in mode but got {mode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_buffer_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0m_check_seekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mraise_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seek\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mraise_err_msg\u001b[0;34m(patterns, e)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                 \u001b[0;34m+\u001b[0m \u001b[0;34m\" Please pre-load the data into a buffer like io.BytesIO and\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                                 + \" try to load from it instead.\")\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ray._raylet.ObjectRef' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "hp_opt.best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrn(dls,config):\n",
    "    lrn = RNNLearner(dls,hidden_size=config['hidden_size'],metrics=[fun_rmse,mse])\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReporter(Callback):\n",
    "    \"`Callback` reports progress after every epoch to the ray tune logger\"\n",
    "\n",
    "    def after_epoch(self):\n",
    "        train_loss,valid_loss,rmse,mse = self.learn.recorder.values[-1]\n",
    "        tune.report(train_loss=train_loss,\n",
    "                        valid_loss=valid_loss,\n",
    "                        mean_loss=rmse,\n",
    "                        mse=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_opt = HPOptimizer(create_lrn,dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-08 00:34:17,033\tINFO services.py:1166 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "hp_opt.start_ray(local_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"hidden_size\": tune.grid_search([10,20,50,100]),\n",
    "    'n_epoch':10,\n",
    "    'reporter':ray.put(CustomReporter)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25333)\u001b[0m Better model found at epoch 9 with valid_loss value: 0.005376223940402269.\n",
      "\u001b[2m\u001b[36m(pid=25333)\u001b[0m /home/daniel/ray_results/learner_optimize/learner_optimize_ba8f5_00003_3_hidden_size=100_2020-12-08_00-34-19/checkpoint_9/model.pth\n",
      "\u001b[2m\u001b[36m(pid=25333)\u001b[0m /home/daniel/ray_results/learner_optimize/learner_optimize_ba8f5_00003_3_hidden_size=100_2020-12-08_00-34-19/checkpoint_10/model.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 19.6/125.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/64 CPUs, 0.0/2 GPUs, 0.0/72.27 GiB heap, 0.0/24.12 GiB objects (0/1.0 accelerator_type:RTX)<br>Result logdir: /home/daniel/ray_results/learner_optimize<br>Number of trials: 4 (4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  valid_loss</th><th style=\"text-align: right;\">       mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>learner_optimize_ba8f5_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">0.240237 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         31.5604</td><td style=\"text-align: right;\">   0.0539474</td><td style=\"text-align: right;\">  0.0577583 </td><td style=\"text-align: right;\">0.0577583 </td></tr>\n",
       "<tr><td>learner_optimize_ba8f5_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">0.215261 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         31.4976</td><td style=\"text-align: right;\">   0.0478665</td><td style=\"text-align: right;\">  0.0463688 </td><td style=\"text-align: right;\">0.0463688 </td></tr>\n",
       "<tr><td>learner_optimize_ba8f5_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">0.121881 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         31.9954</td><td style=\"text-align: right;\">   0.0250287</td><td style=\"text-align: right;\">  0.0148716 </td><td style=\"text-align: right;\">0.0148716 </td></tr>\n",
       "<tr><td>learner_optimize_ba8f5_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">0.0732954</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         32.299 </td><td style=\"text-align: right;\">   0.0164905</td><td style=\"text-align: right;\">  0.00537622</td><td style=\"text-align: right;\">0.00537622</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f71b8dd5490>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_opt.optimize(resources_per_trial={\"gpu\": 0.5},\n",
    "                config=search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_size': 100,\n",
       " 'n_epoch': 10,\n",
       " 'reporter': ObjectRef(ffffffffffffffffffffffff0100000003000000),\n",
       " 'create_lrn': ObjectRef(ffffffffffffffffffffffff0100000004000000),\n",
       " 'dls': ObjectRef(ffffffffffffffffffffffff0100000005000000),\n",
       " 'fit_method': ObjectRef(ffffffffffffffffffffffff0100000006000000)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_opt.analysis.get_best_config('mean_loss',mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 01a_IndRNN.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
