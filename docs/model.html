---

title: Models
keywords: fastai
sidebar: home_sidebar

summary: "Pytorch Models for Sequential Data"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_model.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">seq</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">SequenceBlock</span><span class="o">.</span><span class="n">from_hdf</span><span class="p">([</span><span class="s1">&#39;current&#39;</span><span class="p">,</span><span class="s1">&#39;voltage&#39;</span><span class="p">],</span><span class="n">TensorSequencesInput</span><span class="p">,</span><span class="n">clm_shift</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
                        <span class="n">SequenceBlock</span><span class="o">.</span><span class="n">from_hdf</span><span class="p">([</span><span class="s1">&#39;voltage&#39;</span><span class="p">],</span><span class="n">TensorSequencesOutput</span><span class="p">,</span><span class="n">clm_shift</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span>
                 <span class="n">get_items</span><span class="o">=</span><span class="n">CreateDict</span><span class="p">([</span><span class="n">DfHDFCreateWindows</span><span class="p">(</span><span class="n">win_sz</span><span class="o">=</span><span class="mi">1000</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">stp_sz</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">clm</span><span class="o">=</span><span class="s1">&#39;current&#39;</span><span class="p">)]),</span>
                 <span class="n">splitter</span><span class="o">=</span><span class="n">ApplyToDict</span><span class="p">(</span><span class="n">ParentSplitter</span><span class="p">()))</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">get_hdf_files</span><span class="p">(</span><span class="s1">&#39;test_data/&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Linear">Linear<a class="anchor-link" href="#Linear">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="SeqLinear" class="doc_header"><code>class</code> <code>SeqLinear</code><a href="https://github.com/daniel-om-weber/seqdata/tree/master/seqdata/model.py#L13" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>SeqLinear</code>(<strong><code>input_size</code></strong>, <strong><code>output_size</code></strong>, <strong><code>hidden_size</code></strong>=<em><code>100</code></em>, <strong><code>hidden_layer</code></strong>=<em><code>1</code></em>, <strong><code>act</code></strong>=<em><code>'Mish'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="RNNs">RNNs<a class="anchor-link" href="#RNNs">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="RNN" class="doc_header"><code>class</code> <code>RNN</code><a href="https://github.com/daniel-om-weber/seqdata/tree/master/seqdata/model.py#L33" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>RNN</code>(<strong><code>input_size</code></strong>, <strong><code>hidden_size</code></strong>, <strong><code>num_layers</code></strong>, <strong><code>hidden_p</code></strong>=<em><code>0.2</code></em>, <strong><code>input_p</code></strong>=<em><code>0.6</code></em>, <strong><code>weight_p</code></strong>=<em><code>0.5</code></em>, <strong><code>rnn_type</code></strong>=<em><code>'gru'</code></em>, <strong><code>ret_full_hidden</code></strong>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>inspired by <a href="https://arxiv.org/abs/1708.02182">https://arxiv.org/abs/1708.02182</a></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="SimpleRNN" class="doc_header"><code>class</code> <code>SimpleRNN</code><a href="https://github.com/daniel-om-weber/seqdata/tree/master/seqdata/model.py#L72" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>SimpleRNN</code>(<strong><code>input_size</code></strong>, <strong><code>output_size</code></strong>, <strong><code>num_layers</code></strong>=<em><code>1</code></em>, <strong><code>hidden_size</code></strong>=<em><code>100</code></em>, <strong><code>lrn_init_state</code></strong>=<em><code>False</code></em>, <strong><code>hidden_p</code></strong>=<em><code>0.2</code></em>, <strong><code>input_p</code></strong>=<em><code>0.6</code></em>, <strong><code>weight_p</code></strong>=<em><code>0.5</code></em>, <strong><code>rnn_type</code></strong>=<em><code>'gru'</code></em>, <strong><code>ret_full_hidden</code></strong>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">lrn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">db</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">loss_func</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>11.780790</td>
      <td>8.118497</td>
      <td>00:04</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">)</span>
<span class="n">lrn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">db</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">loss_func</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>12.369296</td>
      <td>10.865791</td>
      <td>00:04</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;qrnn&#39;</span><span class="p">)</span>
<span class="n">lrn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">db</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">loss_func</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>13.649688</td>
      <td>13.133910</td>
      <td>00:02</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="CNNs">CNNs<a class="anchor-link" href="#CNNs">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="CausalConv1d" class="doc_header"><code>class</code> <code>CausalConv1d</code><a href="https://github.com/daniel-om-weber/seqdata/tree/master/seqdata/model.py#L94" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>CausalConv1d</code>(<strong><code>in_channels</code></strong>, <strong><code>out_channels</code></strong>, <strong><code>kernel_size</code></strong>, <strong><code>stride</code></strong>=<em><code>1</code></em>, <strong><code>dilation</code></strong>=<em><code>1</code></em>, <strong><code>groups</code></strong>=<em><code>1</code></em>, <strong><code>bias</code></strong>=<em><code>True</code></em>) :: <code>Conv1d</code></p>
</blockquote>
<p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
:math:<code>(N, C_{\text{in}}, L)</code> and output :math:<code>(N, C_{\text{out}}, L_{\text{out}})</code> can be
precisely described as:</p>
<p>.. math::
    \text{out}(N<em>i, C</em>{\text{out}<em>j}) = \text{bias}(C</em>{\text{out}<em>j}) +
    \sum</em>{k = 0}^{C<em>{in} - 1} \text{weight}(C</em>{\text{out}_j}, k)
    \star \text{input}(N_i, k)</p>
<p>where :math:<code>\star</code> is the valid <code>cross-correlation</code>_ operator,
:math:<code>N</code> is a batch size, :math:<code>C</code> denotes a number of channels,
:math:<code>L</code> is a length of signal sequence.</p>
<ul>
<li><p>:attr:<code>stride</code> controls the stride for the cross-correlation, a single
number or a one-element tuple.</p>
</li>
<li><p>:attr:<code>padding</code> controls the amount of implicit zero-paddings on both sides
for :attr:<code>padding</code> number of points.</p>
</li>
<li><p>:attr:<code>dilation</code> controls the spacing between the kernel points; also
known as the Ã  trous algorithm. It is harder to describe, but this <code>link</code>_
has a nice visualization of what :attr:<code>dilation</code> does.</p>
</li>
<li><p>:attr:<code>groups</code> controls the connections between inputs and outputs.
:attr:<code>in_channels</code> and :attr:<code>out_channels</code> must both be divisible by
:attr:<code>groups</code>. For example,</p>
<ul>
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= :attr:<code>in_channels</code>, each input channel is convolved with
its own set of filters,
of size
:math:<code>\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</code>.</li>
</ul>
</li>
</ul>
<p>.. note::</p>

<pre><code>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid
`cross-correlation`_, and not a full `cross-correlation`_.
It is up to the user to add proper padding.

</code></pre>
<p>.. note::</p>

<pre><code>When `groups == in_channels` and `out_channels == K * in_channels`,
where `K` is a positive integer, this operation is also termed in
literature as depthwise convolution.

In other words, for an input of size :math:`(N, C_{in}, L_{in})`,
a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments
:math:`(C_\text{in}=C_{in}, C_\text{out}=C_{in} \times K, ..., \text{groups}=C_{in})`.

</code></pre>
<p>.. include:: cudnn_deterministic.rst</p>
<p>Args:
    in_channels (int): Number of channels in the input image
    out_channels (int): Number of channels produced by the convolution
    kernel_size (int or tuple): Size of the convolving kernel
    stride (int or tuple, optional): Stride of the convolution. Default: 1
    padding (int or tuple, optional): Zero-padding added to both sides of
        the input. Default: 0
    padding_mode (string, optional). Accepted values <code>zeros</code> and <code>circular</code> Default: <code>zeros</code>
    dilation (int or tuple, optional): Spacing between kernel
        elements. Default: 1
    groups (int, optional): Number of blocked connections from input
        channels to output channels. Default: 1
    bias (bool, optional): If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
<p>Shape:</p>

<pre><code>- Input: :math:`(N, C_{in}, L_{in})`
- Output: :math:`(N, C_{out}, L_{out})` where

  .. math::
      L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}
                \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor

</code></pre>
<p>Attributes:
    weight (Tensor): the learnable weights of the module of shape
        :math:<code>(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})</code>.
        The values of these weights are sampled from
        :math:<code>\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
        :math:<code>k = \frac{1}{C_\text{in} * \text{kernel\_size}}</code>
    bias (Tensor):   the learnable bias of the module of shape
        (out<em>channels). If :attr:<code>bias</code> is <code>True</code>, then the values of these weights are
        sampled from :math:<code>\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
        :math:`k = \frac{1}{C</em>\text{in} * \text{kernel_size}}`</p>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)
&gt;&gt;&gt; input = torch.randn(20, 16, 50)
&gt;&gt;&gt; output = m(input)

</code></pre>
<p>.. _cross-correlation:
    <a href="https://en.wikipedia.org/wiki/Cross-correlation">https://en.wikipedia.org/wiki/Cross-correlation</a></p>
<p>.. _link:
    <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</a></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="CConv1D" class="doc_header"><code>CConv1D</code><a href="https://github.com/daniel-om-weber/seqdata/tree/master/seqdata/model.py#L119" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>CConv1D</code>(<strong><code>input_size</code></strong>, <strong><code>output_size</code></strong>, <strong><code>kernel_size</code></strong>=<em><code>2</code></em>, <strong><code>activation</code></strong>=<em><code>'Mish'</code></em>, <strong><code>bn</code></strong>=<em><code>True</code></em>, <strong><code>stride</code></strong>=<em><code>1</code></em>, <strong><code>dilation</code></strong>=<em><code>1</code></em>, <strong><code>groups</code></strong>=<em><code>1</code></em>, <strong><code>bias</code></strong>=<em><code>True</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="TCN" class="doc_header"><code>class</code> <code>TCN</code><a href="https://github.com/daniel-om-weber/seqdata/tree/master/seqdata/model.py#L128" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>TCN</code>(<strong><code>input_size</code></strong>, <strong><code>output_size</code></strong>, <strong><code>hl_depth</code></strong>=<em><code>1</code></em>, <strong><code>hl_width</code></strong>=<em><code>10</code></em>, <strong><code>act</code></strong>=<em><code>'Mish'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">TCN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lrn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">db</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">loss_func</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>13.579419</td>
      <td>13.804090</td>
      <td>00:02</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="CRNNs">CRNNs<a class="anchor-link" href="#CRNNs">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="CRNN" class="doc_header"><code>CRNN</code><a href="https://github.com/daniel-om-weber/seqdata/tree/master/seqdata/model.py#L152" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>CRNN</code>(<strong><code>input_size</code></strong>, <strong><code>output_size</code></strong>, <strong><code>num_ft</code></strong>=<em><code>10</code></em>, <strong><code>num_cnn_layers</code></strong>=<em><code>4</code></em>, <strong><code>num_rnn_layers</code></strong>=<em><code>2</code></em>, <strong><code>hs_cnn</code></strong>=<em><code>10</code></em>, <strong><code>hs_rnn</code></strong>=<em><code>10</code></em>, <strong><code>hidden_p</code></strong>=<em><code>0.2</code></em>, <strong><code>input_p</code></strong>=<em><code>0.0</code></em>, <strong><code>weight_p</code></strong>=<em><code>0.5</code></em>, <strong><code>rnn_type</code></strong>=<em><code>'qrnn'</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">CRNN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">lrn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">db</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">loss_func</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>14.173500</td>
      <td>14.236246</td>
      <td>00:02</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">CRNN</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;gru&#39;</span><span class="p">)</span>
<span class="n">lrn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">db</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">loss_func</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>13.809017</td>
      <td>13.273102</td>
      <td>00:03</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Autoregressive-Models">Autoregressive Models<a class="anchor-link" href="#Autoregressive-Models">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="Normalizer1D" class="doc_header"><code>class</code> <code>Normalizer1D</code><a href="https://github.com/daniel-om-weber/seqdata/tree/master/seqdata/model.py#L160" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>Normalizer1D</code>(<strong><code>mean</code></strong>, <strong><code>std</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="AR_Model" class="doc_header"><code>class</code> <code>AR_Model</code><a href="https://github.com/daniel-om-weber/seqdata/tree/master/seqdata/model.py#L175" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>AR_Model</code>(<strong><code>model</code></strong>, <strong><code>ar</code></strong>=<em><code>True</code></em>, <strong><code>rf</code></strong>=<em><code>1</code></em>, <strong><code>hs</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3 id="AR_RNN" class="doc_header"><code>class</code> <code>AR_RNN</code><a href="https://github.com/daniel-om-weber/seqdata/tree/master/seqdata/model.py#L226" class="source_link" style="float:right">[source]</a></h3><blockquote><p><code>AR_RNN</code>(<strong><code>input_size</code></strong>, <strong><code>output_size</code></strong>, <strong><code>num_layers</code></strong>=<em><code>1</code></em>, <strong><code>hidden_size</code></strong>=<em><code>100</code></em>, <strong><code>hidden_p</code></strong>=<em><code>0.2</code></em>, <strong><code>input_p</code></strong>=<em><code>0.6</code></em>, <strong><code>weight_p</code></strong>=<em><code>0.5</code></em>, <strong><code>rnn_type</code></strong>=<em><code>'gru'</code></em>, <strong><code>ret_full_hidden</code></strong>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AR_Model</span><span class="p">(</span><span class="n">AR_RNN</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">ar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">hs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">init_normalize</span><span class="p">(</span><span class="n">db</span><span class="o">.</span><span class="n">one_batch</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>
 

