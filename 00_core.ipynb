{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext line_profiler\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corefunctions\n",
    "> Corefunctionality for data preparation of sequential data for pytorch, fastai models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Structure\n",
    "The data will be extracted and prepared via transforms. Those are grouped in:\n",
    "- Type Transforms: Those extraxt the needed components from the source items, like input sequences or target scalar values. The work on single tensors.\n",
    "- Item Transforms: Those Transforms may work on tuple level and therefore may process relationships between input and output.\n",
    "- Batch Transform: Those transforms work on batch level. They receive batched tensors and may apply lazy transforms like normalization very effeciently.\n",
    "\n",
    "An application example may look like the following:\n",
    "- sourceitems: \n",
    "    - path extraction with hdf5 file endings\n",
    "    - create pandas dataframe with information for type transforms, like slices\n",
    "    - filter items in pandas dataframe\n",
    "- type transforms: \n",
    "    - extract hdf5 input and output sequence\n",
    "    - create windows\n",
    "- item transforms: \n",
    "    - filter sequence by value\n",
    "    - shift output sequence by 1 element\n",
    "- batch transforms: \n",
    "    - noise injection\n",
    "    - normalization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.augment import RandTransform\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def obj_in_lst(lst,cls):\n",
    "    '''retrieve first object of type cls from a list'''\n",
    "    return next(o for o in lst if type(o) is cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def count_parameters(model):\n",
    "    '''retrieve number of trainable parameters of a model'''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Source Items\n",
    "The file paths may be extracted with `get_files` of fastai2. `get_hdf_files` removes the need of writing the hdf5 file extension.\n",
    "\n",
    "Then a pandas dataframe may be created in case further information for the source items need to be stored like slices for the windowing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extract File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, Path('test_data/train/Sim_RealisticCycle1.hdf5'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_path = 'test_data/'\n",
    "hdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "hdf_extensions = ['.hdf5']\n",
    "def get_hdf_files(path,recurse=True, folders=None):\n",
    "    \"Get hdf5 files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=hdf_extensions, recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, Path('test_data/train/Sim_RealisticCycle1.hdf5'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_files = get_hdf_files(f_path)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create Source Dictionaries\n",
    "In order to extract mulitple realizations of one file with different modifications, we create a list of properties. Pandas Dataframes are to slow for iteration but very fast and convenient for creations. So after creation of the pandas Dataframe we convert it to a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_df_tfms(src,pd_tfms = None):\n",
    "    '''Create Pandas Dataframe out of a list of items, with a list of df transforms applied'''\n",
    "    if type(src) is pd.DataFrame:\n",
    "        df = src\n",
    "    else:\n",
    "        df = pd.DataFrame(data=src.items,columns=['path'],dtype=str)\n",
    "    if pd_tfms is not None:\n",
    "        for t in pd_tfms:\n",
    "            df = t(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = apply_df_tfms(hdf_files)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(apply_df_tfms(hdf_files),apply_df_tfms(apply_df_tfms(hdf_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CreateDict(pd_tfms = None):\n",
    "    '''Create List of Dictionarys out of a list of items, with a list of df transforms applied'''\n",
    "    def _inner(src):\n",
    "        df = apply_df_tfms(src,pd_tfms)\n",
    "#         df_dict_list = df.to_dict(orient='records') native to_dict is slower than self written approach\n",
    "        df_values = df.values\n",
    "        df_dict = {name:list(df_values[:,i]) for (i,name) in enumerate(df.columns)}\n",
    "        df_dict_list = [{name: df_dict[name][i] for name in df_dict} for i in range(len(df))]\n",
    "        return df_dict_list\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5'},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle2.hdf5'},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_dict =CreateDict()(hdf_files)\n",
    "l_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ValidClmContains(lst_valid):\n",
    "    '''add validation column using a list of strings that are part of the validation frames'''\n",
    "    def _inner(df):\n",
    "        re_valid = '|'.join([re.escape(f) for f in lst_valid])\n",
    "        df['valid'] = df.path.str.contains(re_valid)\n",
    "        return df\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 ms, sys: 61 Âµs, total: 1.34 ms\n",
      "Wall time: 1.25 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5', 'valid': False},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle2.hdf5', 'valid': False},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5', 'valid': True}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lst_valid = ['valid']\n",
    "CreateDict([ValidClmContains(lst_valid)])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ValidClmIs(lst_valid):\n",
    "    '''adds validation column using a list of validation filenames'''\n",
    "    def _inner(df):\n",
    "        df['valid'] = df.path.isin([str(f) for f in lst_valid])\n",
    "        return df\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 ms, sys: 49 Âµs, total: 1.27 ms\n",
      "Wall time: 1.03 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5', 'valid': False},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle2.hdf5', 'valid': True},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5', 'valid': True}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lst_valid = ['test_data/train/Sim_RealisticCycle2.hdf5','test_data/valid/Sim_RealisticCycle3.hdf5']\n",
    "CreateDict([ValidClmIs(lst_valid)])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def FilterClm(clm_name,func = lambda x:x):\n",
    "    '''adds validation column using a list of validation filenames'''\n",
    "    def _inner(df):\n",
    "        return df[func(df[clm_name])]\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle2.hdf5', 'valid': True},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5', 'valid': True}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CreateDict([ValidClmIs(lst_valid),FilterClm('valid')])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_hdf_seq_len(f_path,clm):\n",
    "    '''extract the sequence length of the dataset with the 'clm' name and 'f_path' path  '''\n",
    "    with h5py.File(f_path,'r') as f:\n",
    "        f_len = max(f[clm].shape)\n",
    "    return f_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def df_get_hdf_seq_len(df,clm):\n",
    "    '''extracts the sequence length of every file in advance to prepare repeated window extractions with 'DfHDFCreateWindows' '''\n",
    "#     df['seq_len'] = ([get_hdf_seq_len(row.path,clm) for (idx, row) in df.iterrows()])\n",
    "    df['seq_len'] = df.path.apply(lambda x: get_hdf_seq_len(x,clm))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfHDFGetSeqLen(clm):\n",
    "    def _inner(df):\n",
    "        return df_get_hdf_seq_len(df,clm)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  seq_len\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5   265607\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5   265598\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5   265593"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_get_hdf_seq_len(df,'current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  seq_len\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5   265607\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5   265598\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5   265593"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DfHDFGetSeqLen('current')(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfHDFCreateWindows(win_sz,stp_sz, clm, fixed_start = False, fixed_end = False):\n",
    "    '''create windows of sequences, splits sequence into multiple items'''\n",
    "    def _inner(df):\n",
    "        if fixed_start and fixed_end: raise Exception\n",
    "        \n",
    "        if 'seq_len' in df:\n",
    "            np_f_len = df.seq_len.values\n",
    "        else:\n",
    "            np_f_len = np.array([get_hdf_seq_len(row.path,clm) for (idx, row) in df.iterrows()])\n",
    "            \n",
    "        n_win = ((np_f_len-win_sz)//stp_sz)+1\n",
    "        n_win = np.clip(n_win,a_min=0,a_max=None) #remove negative values at instances where the winsize is smaller than the seq_len\n",
    "        lst_idx = np.arange(len(np_f_len))\n",
    "        \n",
    "        pd.options.mode.chained_assignment = None #every row is a reference so we need to suppress the warning messages while copying\n",
    "        \n",
    "        res_df = df.iloc[np.repeat(lst_idx,n_win)]\n",
    "#         res_df = df.loc[np.repeat(lst_idx,n_win)] #the loc variant as a little bit slower because it creates copies and returns wrong values with redundant indexes, but is more robust\n",
    "\n",
    "        step_idx = np.concatenate([np.arange(x) for x in n_win])\n",
    "    \n",
    "        \n",
    "        res_df['l_slc'] = step_idx*stp_sz if not fixed_start else None\n",
    "        res_df['r_slc'] = step_idx*stp_sz + win_sz if not fixed_end else None\n",
    "            \n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "            \n",
    "        return res_df\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.9 ms, sys: 206 Âµs, total: 3.11 ms\n",
      "Wall time: 2.03 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>400</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>265000</td>\n",
       "      <td>265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>265100</td>\n",
       "      <td>265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>265200</td>\n",
       "      <td>265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>265300</td>\n",
       "      <td>265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>265400</td>\n",
       "      <td>265500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7966 rows Ã 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        path  seq_len   l_slc   r_slc\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   265607       0     100\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   265607     100     200\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   265607     200     300\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   265607     300     400\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   265607     400     500\n",
       "..                                       ...      ...     ...     ...\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5   265593  265000  265100\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5   265593  265100  265200\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5   265593  265200  265300\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5   265593  265300  265400\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5   265593  265400  265500\n",
       "\n",
       "[7966 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "create_win = DfHDFCreateWindows(win_sz=100,stp_sz=100,clm='current')\n",
    "win_df = create_win(df)\n",
    "win_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  seq_len  l_slc   r_slc\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5   265607      0  265594\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5   265598      0  265594"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_df = DfHDFCreateWindows(win_sz=265594,stp_sz=100,clm='current')(df)\n",
    "test_eq(len(win_df),2)\n",
    "win_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(create_win(df_get_hdf_seq_len(df,'current')) , create_win(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  seq_len  l_slc   r_slc\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5   265607      0  265594\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5   265598      0  265594"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expr = 'l_slc <= 200'\n",
    "filt_df = win_df.query(query_expr)\n",
    "filt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfFilterQuery(query):\n",
    "    def _inner(df):\n",
    "        return df.query(query)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(DfFilterQuery(query_expr)(win_df),filt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.8 ms, sys: 12.2 ms, total: 72 ms\n",
      "Wall time: 70.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 0,\n",
       "  'r_slc': 101},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 10,\n",
       "  'r_slc': 111},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 20,\n",
       "  'r_slc': 121},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 30,\n",
       "  'r_slc': 131},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 40,\n",
       "  'r_slc': 141}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='current')])\n",
    "src_dicts = tfm_src(hdf_files)\n",
    "src_dicts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfDropClmExcept(clms = ['path','l_slc','r_slc','p_sample']):\n",
    "    '''drop unused dataframe columns as a last optional step to accelerate dictionary conversion'''\n",
    "    def _inner(df):\n",
    "        return df[[c for c in clms if c in df]]\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfHDFCreateSamples(win_sz,stp_sz, clm, rpt_sz=False):\n",
    "    '''create windows of sequences, splits sequence into multiple items'''\n",
    "    def _inner(df):        \n",
    "        if 'seq_len' in df:\n",
    "            np_f_len = df.seq_len.values\n",
    "        else:\n",
    "            np_f_len = np.array([get_hdf_seq_len(row.path,clm) for (idx, row) in df.iterrows()])\n",
    "        \n",
    "        # Calculate sequence length\n",
    "        n_spl = np_f_len // rpt_sz\n",
    "        \n",
    "        # Calculate number of windows per repition\n",
    "        n_win = ((np_f_len//n_spl-win_sz)//stp_sz)+1 \n",
    "        lst_idx = np.arange(len(np_f_len))\n",
    "        \n",
    "        pd.options.mode.chained_assignment = None #every row is a reference so we need to suppress the warning messages while copying\n",
    "        \n",
    "        res_df = df.iloc[np.repeat(lst_idx,n_win)]\n",
    "\n",
    "        step_idx = np.concatenate([np.arange(x) for x in n_win])\n",
    "        # Create the index for the repition\n",
    "        rpt_idx = np.concatenate([np.repeat(a,b)  for a,b in zip(n_spl,n_win)])\n",
    "        \n",
    "        res_df['l_slc'] = [np.arange(a)*rpt_sz + b*stp_sz for a,b in zip(rpt_idx,step_idx)]\n",
    "        res_df['r_slc'] = [np.arange(a)*rpt_sz + b*stp_sz + win_sz for a,b in zip(rpt_idx,step_idx)]\n",
    "        res_df['rpt_sz'] = [rpt_sz for i in step_idx]\n",
    "            \n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "            \n",
    "        return res_df\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.72 ms, sys: 1.16 ms, total: 3.87 ms\n",
      "Wall time: 2.67 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "      <th>rpt_sz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>[0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, ...]</td>\n",
       "      <td>[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, ...]</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "      <td>[0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, ...]</td>\n",
       "      <td>[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, ...]</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>[0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, ...]</td>\n",
       "      <td>[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, ...]</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  seq_len  \\\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5   265607   \n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5   265598   \n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5   265593   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               l_slc  \\\n",
       "0  [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, ...]   \n",
       "1  [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, ...]   \n",
       "2  [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, ...]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   r_slc  \\\n",
       "0  [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, ...]   \n",
       "1  [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, ...]   \n",
       "2  [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, ...]   \n",
       "\n",
       "   rpt_sz  \n",
       "0     100  \n",
       "1     100  \n",
       "2     100  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "create_win = DfHDFCreateSamples(win_sz=100,stp_sz=100, rpt_sz=100, clm='current')\n",
    "win_df = create_win(df)\n",
    "win_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Paths to Sequence Objects\n",
    "Der Pfad wird unter Angabe der Spaltennamen in Sequenzen und Skalare Werte umgewandelt, um so am Ende ein 3-Tupel zu erhalten aus:\n",
    "- (Sequence, Scalar, Sequence) <-> (input,input,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract sequential data from hdf5-files\n",
    "Two different functions, based on pandas df and on lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Shift time Series\n",
    "Sometimes we need to shift columns of a sequence by a specific value. Then we cant simply slice the array but have to handle each column individually. First a performance test has to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_shift_offsets(clm_shift):\n",
    "    clm_shift = array(clm_shift)\n",
    "    l_offs = -min(clm_shift.min(),0)\n",
    "    r_offs = -max(clm_shift.max(),0)\n",
    "    l_shift = clm_shift+l_offs\n",
    "    r_shift = clm_shift+r_offs\n",
    "    dim_red = l_offs-r_offs\n",
    "    return l_shift,r_shift,dim_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 2]), array([-1, -1, -2,  0]), 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shft = [0,0,-1,1]\n",
    "calc_shift_offsets(shft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both shifting methods have their own performance character. vstack needs double the time on short sequences, while the creation of a seperate array with copy becomes worse starting at around 5000 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta = array([[1,2,3]*2]*10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# y = np.vstack([ta[i:-ta.shape[1]+i,i] for i in range(ta.shape[1])]).T   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# x = np.zeros((ta.shape[0]-ta.shape[1],ta.shape[1]))\n",
    "# for i in range(ta.shape[1]):\n",
    "#     x[:,i] = ta[i:-ta.shape[1]+i,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 HDF2Sequence\n",
    "HDF5 performance is massively affected by the dtype of the signals. f4 (32 bit floating point) Numbers are faster to load and lead to smaller files then f8 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0,axis=0),axis=0) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def downsample_mean(x,N):\n",
    "    shp = x.shape\n",
    "    trunc = -(x.shape[0] % N)\n",
    "    trunc = trunc if trunc != 0 else None\n",
    "    return x[:trunc,:].reshape((-1,N,x.shape[-1])).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hdf_extract_sequence(hdf_path,clms,dataset = None, l_slc = None, r_slc= None):\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "        ds = f if dataset is None else f[dataset]\n",
    "        l_array = [ds[n][l_slc:r_slc] for n in clms]\n",
    "        seq = np.stack(l_array,axis=-1)\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Memoize:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "        self.memo = {}\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        if args not in self.memo:\n",
    "            self.memo[args] = self.fn(*args)\n",
    "        return self.memo[args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HDF2Sequence(Transform):\n",
    "    \n",
    "    def __init__(self, clm_names,clm_shift=None,truncate_sz=None,to_cls=noop,cached=False):\n",
    "        if not clm_shift is None:\n",
    "            assert len(clm_shift)==len(clm_names) and all(isinstance(n, int) for n in clm_shift)\n",
    "            self.l_shift,self.r_shift,_ = calc_shift_offsets(clm_shift)\n",
    "        \n",
    "        self._exseq = Memoize(self._hdf_extract_sequence) if cached else self._hdf_extract_sequence\n",
    "        \n",
    "        store_attr('clm_names,clm_shift,truncate_sz,to_cls,cached')\n",
    "        \n",
    "    def _hdf_extract_sequence(self,hdf_path,dataset = None, l_slc = None, r_slc= None, rpt_sz = None):\n",
    "        with h5py.File(hdf_path,'r') as f:\n",
    "            ds = f if dataset is None else f[dataset]\n",
    "            #import pdb; pdb.set_trace()\n",
    "            if rpt_sz:\n",
    "                l_array = [np.stack([ds[n][l:r] for l,r in zip(l_slc,r_slc)]) for n in self.clm_names]\n",
    "                seq = np.stack(l_array,axis=-1)\n",
    "            else:\n",
    "                l_array = [(ds[n][l_slc:r_slc]) for n in self.clm_names]\n",
    "                seq = np.stack(l_array,axis=-1)\n",
    "            return seq\n",
    "    \n",
    "    def _extract_dict_sequence(self,item):\n",
    "        if isinstance(item,dict):\n",
    "            path = item['path']\n",
    "            dataset = item['dataset'] if 'dataset' in item else None\n",
    "            l_slc = item['l_slc'] if 'l_slc' in item else None\n",
    "            r_slc = item['r_slc'] if 'r_slc' in item else None\n",
    "            rpt_sz = item['rpt_sz'] if 'rpt_sz' in item else None\n",
    "\n",
    "            if self.cached:\n",
    "                seq = self._exseq(path,dataset,None,None)[l_slc:r_slc]\n",
    "            else:\n",
    "                seq = self._exseq(path,dataset,l_slc,r_slc,rpt_sz)\n",
    "        else:\n",
    "            seq = self._exseq(str(item),None,None,None)\n",
    "\n",
    "        #shift clms of result by given value \n",
    "        if not self.clm_shift is None:\n",
    "            l_seq = seq.shape[0]\n",
    "            seq = np.stack([seq[self.l_shift[i]:l_seq+self.r_shift[i],i] for i in range(seq.shape[1])],axis=-1)\n",
    "            \n",
    "        if not self.truncate_sz is None:\n",
    "            seq = seq[truncate_sz:]\n",
    "        \n",
    "        #it is important to slice first and then do the class conversion\n",
    "#         return self.to_cls(seq.astype('f8'))#workaround for random bug, that mitigates convergence if the numpy array is an f4 array. Seems to make no sense because the result does not change. \n",
    "        return self.to_cls(seq)\n",
    "\n",
    "    def encodes(self, item)->None: \n",
    "        return self._extract_dict_sequence(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265607, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=False)\n",
    "hdf2seq(hdf_files[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1      ,  4.18935  ],\n",
       "       [-0.1      ,  4.1896954],\n",
       "       [-0.1      ,  4.190037 ],\n",
       "       ...,\n",
       "       [ 8.8388   ,  3.3932123],\n",
       "       [ 8.846    ,  3.3928714],\n",
       "       [ 8.8531   ,  3.3925302]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],clm_shift=[1,1])\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  4.1873503],\n",
       "       [-0.1      ,  4.18935  ],\n",
       "       [-0.1      ,  4.1896954],\n",
       "       ...,\n",
       "       [ 8.8388   ,  3.3932123],\n",
       "       [ 8.846    ,  3.3928714],\n",
       "       [ 8.8531   ,  3.3925302]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=False)\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.5 ms Â± 501 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  4.1873503],\n",
       "       [-0.1      ,  4.18935  ],\n",
       "       [-0.1      ,  4.1896954],\n",
       "       ...,\n",
       "       [ 8.8388   ,  3.3932123],\n",
       "       [ 8.846    ,  3.3928714],\n",
       "       [ 8.8531   ,  3.3925302]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion lÃ¤sst sich mittels Pipeline auf eine Liste von Quellobjekten (hier Pfade) anwenden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265607, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current'])\n",
    "hdf2seq(hdf_files[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(HDF2Sequence(['current','voltage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_pipe = pipe(hdf_files)\n",
    "# len(res_pipe), res_pipe[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hdf2scalars(hdf_path,c_names):\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         l_array = [f[n][:][:,None] for n in c_names]\n",
    "#         seq = np.concatenate(l_array,axis=1)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Test\n",
    "Caching stores the arrays for future use at every function call. Very usefull, especially for windows. Should allways be turned. Only explicitly turn it off when there is not enough memory for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage'],cached=False)],\n",
    "        [HDF2Sequence(['voltage'],cached=False)]]\n",
    "dsrc = Datasets(src_dicts[:1000],tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for x in dsrc:\n",
    "#     x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage'],cached=True,clm_shift=[1,2])],\n",
    "        [HDF2Sequence(['voltage'],cached=True)]]\n",
    "dsrc = Datasets(src_dicts[:1000],tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "for x in dsrc:\n",
    "    x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is way faster because every file gets loaded multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Datatypes for Sequences and Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorSequences(TensorBase):#TensorBase\n",
    "#     def __init__(self,x,c_names=None, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.c_names = c_names\n",
    "    \n",
    "    def show(self, ctx=None, **kwargs):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        ax = ctx\n",
    "        if ax is None: _,ax = plt.subplots()\n",
    "        ax.plot(self)\n",
    "#         if title is not None: ax.set_title(title)\n",
    "        return ax\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Sequence, keep=True)\n",
    "    def from_hdf(cls,clm_names,**kwargs):\n",
    "        return HDF2Sequence(clm_names,**kwargs)\n",
    "    \n",
    "class TensorSequencesInput(TensorSequences): pass\n",
    "class TensorSequencesOutput(TensorSequences): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = TensorSequencesInput.from_hdf(['current'])\n",
    "type(f(hdf_files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorSequences(np.ones((30,2))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Transform\n",
    "def toTensorSequencesInput(o): return TensorSequencesInput(o)\n",
    "@Transform\n",
    "def toTensorSequencesOutput(o): return TensorSequencesOutput(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorScalars(Tensor): pass\n",
    "class TensorScalarsInput(TensorScalars): pass\n",
    "class TensorScalarsOutput(TensorScalars): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sequence Slicing Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class SeqSlice(Transform):\n",
    "    '''Take a slice from an array-like object. Useful for e.g. shifting input and output'''\n",
    "    def __init__(self, l_slc=None,r_slc=None):\n",
    "        self.l_slc,self.r_slc = l_slc,r_slc\n",
    "        \n",
    "    def encodes(self, o): return o[self.l_slc:self.r_slc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_shift = SeqSlice(r_slc=-1)\n",
    "arr = np.ones((5))\n",
    "test_eq(l_shift(arr),arr[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Noise Injection Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqNoiseInjection(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds normal distributed noise to the tensor sequence with seperate mean and std for every signal'''\n",
    "    def __init__(self, std=1e-1,mean=0.,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std = tensor(std).type(torch.float)\n",
    "        self.mean = tensor(mean).type(torch.float)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.mean.device:\n",
    "            self.std = self.std.to(o.device)\n",
    "            self.mean = self.mean.to(o.device)\n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        return o+torch.normal(mean=self.mean.expand_as(o), \n",
    "                              std=self.std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "         [-1., -1., -1.]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TensorSequencesInput(tensor([[1,1,1],[-1,-1,-1.0]]))\n",
    "ns_mean = tensor([0.,10.1,3.1])\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "        [-1., -1., -1.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection(std=ns_std,mean=ns_mean)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "        [-1., -1., -1.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection(std=ns_std*10)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqNoiseInjection_Varying(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds normal distributed noise to the tensor sequence with a normal distributed standard deviation for every application'''\n",
    "    def __init__(self, std_std=0.1,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std_std = tensor(std_std).type(torch.float)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.std_std.device:\n",
    "            self.std_std = self.std_std.to(o.device)\n",
    "            \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        std = torch.normal(mean=0,std=self.std_std).abs()\n",
    "        return o+torch.normal(mean=0,std=std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSequencesInput([[0, 0, 0],\n",
       "         [0, 0, 0]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TensorSequencesInput(tensor([[0,0,0],[0,0,0]]))\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection_Varying(std_std=ns_std)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqNoiseInjection_Grouped(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds normal distributed noise to the tensor sequence with a normal distributed standard deviation for every application, every group gert'''\n",
    "    def __init__(self, std_std,std_idx,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std_std = tensor(std_std).type(torch.float)\n",
    "        self.std_idx = tensor(std_idx).type(torch.long)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.std_std.device:\n",
    "            self.std_std = self.std_std.to(o.device)\n",
    "            \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        std = torch.normal(mean=0,std=self.std_std).abs()[self.std_idx]\n",
    "        return o+torch.normal(mean=0,std=std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSequencesInput([[0, 0, 0],\n",
       "         [0, 0, 0]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TensorSequencesInput(tensor([[0,0,0],[0,0,0]]))\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection_Grouped(std_std=[3.,0],std_idx=[0,0,1])\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Bias Injection Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqBiasInjection(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds a normal distributed offset to the tensor sequence with seperate mean and std for every signal'''\n",
    "    def __init__(self, std=1e-1,mean=0.,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std = tensor(std).type(torch.float)\n",
    "        self.mean = tensor(mean).type(torch.float)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.mean.device:\n",
    "            self.std = self.std.to(o.device)\n",
    "            self.mean = self.mean.to(o.device)\n",
    "        \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        mean=self.mean.repeat((o.shape[0],1,1)).expand((o.shape[0],1,o.shape[2]))\n",
    "        std= self.std.repeat((o.shape[0],1,1)).expand((o.shape[0],1,o.shape[2]))\n",
    "        n = torch.normal(mean=mean, std=std).expand_as(o)\n",
    "        return o+n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[ 1., -1.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TensorSequencesInput(tensor([[[1,1,1],[-1,-1,-1.0]]]))\n",
    "ns_mean = tensor([0.,10.1,3.1])\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "seq_bias = SeqBiasInjection(std=ns_std,mean=ns_std)\n",
    "seq_bias(x)[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.1000, 0.1000])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_bias.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[[ 1.,  1.,  1.],\n",
       "         [-1., -1., -1.]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_bias = SeqBiasInjection(std=ns_std*10)\n",
    "seq_bias(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Normalization\n",
    "`Normalize` is programmed for `TensorImage` as an input tensor. It gets. At init the variable axes need to be chosen correspondingly to the shape of your tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Normalize\n",
    "def encodes(self, x:TensorSequencesInput): \n",
    "    if x.device != self.mean.device:\n",
    "        self.mean = self.mean.to(x.device)\n",
    "        self.std = self.std.to(x.device)\n",
    "    return (x-self.mean) / self.std\n",
    "\n",
    "@Normalize\n",
    "def decodes(self, x:TensorSequencesInput):\n",
    "    if x.device != self.mean.device:\n",
    "        self.mean = self.mean.to(x.device)\n",
    "        self.std = self.std.to(x.device)\n",
    "    return (x*self.std + self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSequencesInput([[[ 1.,  1.,  1.],\n",
       "          [-1., -1., -1.]]]),\n",
       " TensorSequencesInput([[[  1.0000,  -8.2727, -21.0000],\n",
       "          [ -1.0000, -10.0909, -41.0000]]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = Normalize.from_stats(mean=ns_mean,std=ns_std,dim=1,ndim=2,cuda=False)\n",
    "x,norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split in Training, Validation\n",
    "Splitting kann anhand von vorher bekannten Indizes, dem Dateipfad oder anderen allgemeinen Funktion durchgefÃ¼hrt werden.\n",
    "\n",
    "Splitting innerhalb einer Sequenzen sollte in der Praxis nur dann geschehen wenn eine einzige Sequenz vorhanden ist. Diese kann dann vorher manuell geteilt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Splitting mit vorgegebenem Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = IndexSplitter([1,2])\n",
    "test_eq(splitter(hdf_files),[[0],[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Splitting mit allgemeiner Funktion\n",
    "Items, bei denen die definierte Funktion `True` zurÃ¼ck gibt, werden den Validierungsdatensatz zugeordnet, der Rest dem Training. In diesem Fall wird nach dem Ãbergeordneten Ordnernamen gesucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Splitting anhand des Parent-Folders\n",
    "Splitter, der Explizit Training und Validierungsordner den DatensÃ¤tzen zuordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _parent_idxs(items, name): return mask2idxs(Path(o).parent.name == name for o in items)\n",
    "\n",
    "def ParentSplitter(train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the parent folder names (`train_name` and `valid_name`).\"\n",
    "    def _inner(o, **kwargs):\n",
    "        return _parent_idxs(o, train_name),_parent_idxs(o, valid_name)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ParentSplitter()\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Percentage Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def PercentageSplitter(pct=0.8):\n",
    "    \"Split `items` in order in relative quantity.\"\n",
    "    def _inner(o, **kwargs):\n",
    "        split_idx=int(len(o)*pct)\n",
    "        return L(range(split_idx)),L(range(split_idx,len(o)))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = PercentageSplitter(0.7)\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Apply To Dictionary\n",
    "In Case of the Datablock API your items are a list of dictionaries. If you want to apply a Splitter to the path stored within you need a wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ApplyToDict(fn,key='path'):\n",
    "    return lambda x:fn([i[key] for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Valid Column\n",
    "Using the 'valid' column of the Dataframe that has been created by a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "valid_clm_splitter =  FuncSplitter(lambda o:o['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#53101) [0,1,2,3,4,5,6,7,8,9...],\n",
       " (#26550) [53101,53102,53103,53104,53105,53106,53107,53108,53109,53110...])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_clm_splitter(src_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataloaders Creation\n",
    "A Datasets combines all implemented components on item level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_sequence(batch,sorting = False):\n",
    "    '''collate_fn for padding of sequences of different lengths, use in before_batch of databunch, still quite slow'''\n",
    "    #takes list of tuples as input, returns list of tuples\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True) if sorting else batch\n",
    "\n",
    "    pad_func = partial(torch.nn.utils.rnn.pad_sequence,batch_first=True)\n",
    "    padded_tensors = [pad_func([x[tup] for x in sorted_batch]) for tup in range(len(batch[0]))]\n",
    "    padded_list = [retain_types(tuple([tup[entry] for tup in padded_tensors]),batch[0]) for entry in range(len(batch))]\n",
    "    #retain types is important for decoding later back to source items\n",
    "#     import pdb; pdb.set_trace()\n",
    "    \n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Low-Level with Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage']),SeqSlice(l_slc=1),toTensorSequencesInput],\n",
    "        [HDF2Sequence(['voltage']),SeqSlice(r_slc=-1),toTensorSequencesOutput]]\n",
    "splits = splitter([x['path'] for x in src_dicts])\n",
    "dsrc = Datasets(src_dicts,tfms=tfms,splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# dsrc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100, 2])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = dsrc.dataloaders(bs=128,after_batch=[SeqNoiseInjection(std=[1.1,0.01]),Normalize(axes=[0,1])],before_batch=pad_sequence)\n",
    "db.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Mid-Level with Datablock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceBlock(TransformBlock):\n",
    "    def __init__(self, seq_extract,padding=False):\n",
    "        return super().__init__(type_tfms=[seq_extract],\n",
    "                                batch_tfms=[Normalize(axes=[0,1])],\n",
    "                                dls_kwargs={} if not padding else {'before_batch': pad_sequence})\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Sequence, keep=True)\n",
    "    def from_hdf(cls, clm_names, seq_cls=TensorSequencesInput,padding=False, **kwargs):\n",
    "        return cls(HDF2Sequence(clm_names,to_cls=seq_cls,**kwargs), padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,padding=True,cached=False),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,cached=False)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(ParentSplitter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = seq.dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = pickle.dumps(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 High-Level with SequenceDataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import Dataset,ConcatDataset\n",
    "class Seq2SeqDS(Dataset):\n",
    "    #Datensatz aus HDF5 Datei \n",
    "\n",
    "    def __init__(self, hdf_file, u_clms,y_clms, win_sz,stp_sz):\n",
    "        super().__init__()\n",
    "        store_attr('u_clms,y_clms,win_sz,stp_sz')\n",
    "        \n",
    "        self.u = hdf_extract_sequence(hdf_file,u_clms)\n",
    "        self.y = hdf_extract_sequence(hdf_file,y_clms)\n",
    "        \n",
    "#         if clm_shift is not None:\n",
    "#             assert len(clm_shift[0])==len(u_clms) and len(clm_shift[1])==len(y_clms)\n",
    "#             l_shift,r_shift,dim_red = calc_shift_offsets(clm_shift)\n",
    "        \n",
    "        self.len = (self.u.shape[0]-self.win_sz)//self.stp_sz or 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not (0 <= idx < len(self)): raise IndexError\n",
    "#         import pdb; pdb.set_trace()\n",
    "        i = idx*self.stp_sz + self.win_sz\n",
    "        return (TensorSequencesInput(self.u[(i-self.win_sz+1):i+1]),\n",
    "                TensorSequencesOutput(self.y[(i-self.win_sz+1):i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TfmdDL, keep=True)\n",
    "def Seq2SeqDataloaders(items,u_clms,y_clms, win_sz,stp_sz=1,splitter=None,**kwargs):\n",
    "    if splitter is None:\n",
    "        lst_items = [items]\n",
    "    else:\n",
    "        lst_items = [items[idx] for idx in splitter(items)]\n",
    "        \n",
    "    dss = [ConcatDataset([Seq2SeqDS(f,u_clms,y_clms, win_sz,stp_sz) for f in itms])\n",
    "          for itms in lst_items]\n",
    "    if not 'after_batch' in kwargs: kwargs['after_batch'] = [to_device,Normalize(axes=[0,1])]\n",
    "    \n",
    "    dls = [TfmdDL(ds,shuffle=(i == 0),**kwargs) for i,ds in enumerate(dss)]\n",
    "    return DataLoaders(*dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls=Seq2SeqDataloaders(hdf_files,['current','voltage'],['voltage'],100,10,\n",
    "                       splitter=ParentSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Show Batches and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_sequence(axs,in_sig,targ_sig,out_sig=None,**kwargs):\n",
    "    n_targ = targ_sig.shape[1]\n",
    "    for j,ax in  enumerate(axs[:-1]):\n",
    "        ax.plot(targ_sig[:,j])\n",
    "        if out_sig is not None: \n",
    "            ax.plot(out_sig[:,j])\n",
    "            ax.legend(['y','Å·'])\n",
    "            if 'ref' in kwargs:\n",
    "                ax.plot(kwargs['ref'][:,j]) \n",
    "        ax.label_outer()\n",
    "    axs[-1].plot(in_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_seqs_single_figure(n_samples,n_targ,samples,plot_func,outs=None,**kwargs):\n",
    "    rows=max(1,((n_samples-1) // 3)+1)\n",
    "    cols=min(3,n_samples)\n",
    "    fig = plt.figure(figsize=(9,2*cols))\n",
    "    outer_grid = fig.add_gridspec(rows, cols)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i in range(n_samples):\n",
    "        in_sig = samples[i][0]\n",
    "        targ_sig = samples[i][1]\n",
    "        if outs is not None: out_sig = outs[i][0]\n",
    "        inner_grid = outer_grid[i].subgridspec(n_targ+1, 1)\n",
    "        axs = [fig.add_subplot(inner_grid[j]) for j in range(n_targ+1)]\n",
    "        plot_func(axs,in_sig,targ_sig,out_sig=out_sig if outs is not None else None,**kwargs)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_seqs_multi_figures(n_samples,n_targ,samples,plot_func,outs=None,**kwargs):\n",
    "    for i in range(n_samples):\n",
    "        fig = plt.figure(figsize=(9,3))\n",
    "        axs = fig.subplots(nrows=n_targ+1,sharex=True)\n",
    "        in_sig = samples[i][0]\n",
    "        targ_sig = samples[i][1]\n",
    "        if outs is not None:  out_sig = outs[i][0]\n",
    "            \n",
    "        plot_func(axs,in_sig,targ_sig,out_sig=out_sig if outs is not None else None,**kwargs)\n",
    "        \n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:TensorSequences, y:TensorSequences, samples, ctxs=None, max_n=6, **kwargs):\n",
    "    n_samples = min(len(samples), max_n)\n",
    "    n_targ = samples[0][1].shape[1]\n",
    "    if n_samples > 3:\n",
    "        #if there are more then 3 samples to plot then put them in a single figure\n",
    "        plot_seqs_single_figure(n_samples,n_targ,samples,plot_sequence, **kwargs)\n",
    "    else:\n",
    "        #if there are less then 3 samples to plot then put each in its own figure\n",
    "        plot_seqs_multi_figures(n_samples,n_targ,samples,plot_sequence, **kwargs)\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82f70f581a64c20aeb693fc588c533e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8954472297804423a5bfd76b5a05124f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47df382ccc442b68977b83dea8eabc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:TensorSequences, y:TensorSequences, samples, outs, ctxs=None, max_n=2, **kwargs):\n",
    "    n_samples = min(len(samples), max_n)\n",
    "    n_targ = samples[0][1].shape[1]\n",
    "    if n_samples > 3:\n",
    "        #if there are more then 3 samples to plot then put them in a single figure\n",
    "        plot_seqs_single_figure(n_samples,n_targ,samples,plot_sequence,outs, **kwargs)\n",
    "    else:\n",
    "        #if there are less then 3 samples to plot then put each in its own figure\n",
    "        plot_seqs_multi_figures(n_samples,n_targ,samples,plot_sequence,outs, **kwargs)\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 01a_IndRNN.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
