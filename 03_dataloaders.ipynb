{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataloaders\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%matplotlib notebook\n",
    "from fastai2.callback.progress import *\n",
    "from fastai2.callback.tracker import *\n",
    "from fastai2.callback.schedule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from seqdata.core import *\n",
    "from seqdata.model import *\n",
    "from seqdata.learner import *\n",
    "from fastai2.basics import *\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataloaders\n",
    "> Pytorch Modules for Training Models for sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Backpropagation Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tbptt dataloader needs to split the minibatches that are created in several smaller minibatches that will be returned sequentially before the next minibatch may be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates()\n",
    "class TbpttDl(TfmdDL):\n",
    "\n",
    "    def __init__(self, dataset, sub_seq_len=None,max_batches=None, seq_len = None ,shuffle=True,num_workers=0, **kwargs):\n",
    "        store_attr(self,'sub_seq_len,max_batches,seq_len')\n",
    "        super().__init__(dataset=dataset, shuffle=shuffle, num_workers=num_workers, **kwargs)\n",
    "\n",
    "        self.rnn_reset = sub_seq_len is None #always reset stateful rnns if there are no subsequences\n",
    "    @property\n",
    "    def n_sub_seq(self):\n",
    "        if self.seq_len is None: self.seq_len = self.do_item(0)[0].shape[0]\n",
    "        return math.ceil(self.seq_len / self.sub_seq_len)\n",
    "        \n",
    "    def __len__(self):\n",
    "        l = super().__len__()\n",
    "        if self.sub_seq_len is not None: l *= self.n_sub_seq\n",
    "        if self.max_batches is not None: l = min(l,self.max_batches)\n",
    "        return l\n",
    "    \n",
    "    def create_batches(self, samps):\n",
    "        yield from self._tbptt_generator(super().create_batches(samps))\n",
    "        \n",
    "    def _tbptt_generator(self,batch_iter):\n",
    "        '''generator function that splits batches in smaller windows and truncates batch count if max_batches is set'''\n",
    "        for idx,b in enumerate(batch_iter):\n",
    "            if self.sub_seq_len is None:\n",
    "                self.rnn_reset = True\n",
    "                if self.max_batches is not None and idx >= self.max_batches: return\n",
    "                yield b\n",
    "            else:\n",
    "                for i in range(self.n_sub_seq):\n",
    "                    if self.max_batches is not None and ((idx*self.n_sub_seq)+i) >= self.max_batches: return\n",
    "                    self.rnn_reset = i == 0\n",
    "                    #it is importan to retain the tuple type, or future transforms may now work\n",
    "                    trunc_b = tuple([retain_type(x[:,i*self.sub_seq_len:(i+1)*self.sub_seq_len],x) for x in b])\n",
    "                    yield trunc_b\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_lst = [DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='current')]\n",
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,clm_shift=[-1,-1]),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,clm_shift=[1])),\n",
    "                 get_items=CreateDict(tfm_lst),\n",
    "                 splitter=ApplyToDict(ParentSplitter()))\n",
    "db = seq.dataloaders(get_hdf_files('test_data/'),dl_type=TbpttDl,sub_seq_len=10,max_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10, 2]), torch.Size([64, 1000, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.train.one_batch()[0].shape,db.valid.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_workers has to be 0. If there are parallel workers, the order of minibatches will be corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBPTT_Reset_Callback\n",
    "The stateful model needs to reset its hidden state, when a new sequence begins. The callback reads the reset flag and acts accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TbpttResetCB(Callback):\n",
    "    \"`Callback` resets the rnn model with every new sequence for tbptt\"\n",
    "        \n",
    "    def begin_batch(self):\n",
    "        dl = self.learn.dls.train if self.training else self.learn.dls.valid\n",
    "#         if not self.training: import pdb; pdb.set_trace()\n",
    "        if hasattr(dl,'rnn_reset')and dl.rnn_reset and hasattr(self.model,'reset'):\n",
    "            self.model.reset()\n",
    "        \n",
    "    def after_fit(self): \n",
    "        if hasattr(self.model,'reset'): self.model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai2.learner.Learner at 0x7f8778937668>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrn = RNNLearner(db,num_layers=1,rnn_type='gru',stateful=False,metrics=[SkipNLoss(fun_rmse,100)])\n",
    "lrn.add_cb(TbpttResetCB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.254913</td>\n",
       "      <td>14.269542</td>\n",
       "      <td>3.765729</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrn.fit_one_cycle(1,lr_max=3e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.train.max_batches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.train.sub_seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.725590</td>\n",
       "      <td>0.018130</td>\n",
       "      <td>0.110914</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrn.fit_one_cycle(1,lr_max=3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Sampling Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weighted sampling dataloader for nonuniforly distributed data. A factory method receives the base Dataloader class and returns the inherited weighted sampling dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def WeightedDL_Factory(cls):\n",
    "    '''\n",
    "    Weighted Dataloader that provides control over sampling probabilities.\n",
    "    wgts: probability array with probability for every item\n",
    "            gets extracted from the pandas 'p_sample' column if given. \n",
    "            Otherwise uniform sampling will be enabled\n",
    "        \n",
    "    '''\n",
    "    assert issubclass(cls, TfmdDL)\n",
    "    \n",
    "    class WeightedDL(cls):\n",
    "        def __init__(self, dataset, wgts=None, **kwargs):\n",
    "#             import pdb;pdb.set_trace()\n",
    "            \n",
    "            if wgts is None:\n",
    "                self.wgts = array([1/(len(dataset))]*len(dataset))\n",
    "                #self.items need to be assigned, but super.init needs wgts allready assigned\n",
    "                super().__init__(dataset=dataset, **kwargs)\n",
    "                if  (type(self.items) is list and\n",
    "                    type(self.items[0]) is dict and \n",
    "                    'p_sample' in self.items[0].keys()):\n",
    "                    self.wgts = np.array([x['p_sample'] for x in self.items])\n",
    "                    self.wgts = self.wgts/self.wgts.sum()\n",
    "                else:\n",
    "                    print('No wgts provided for WeightedDL. Was that intentional?')\n",
    "            else:\n",
    "                self.wgts = wgts/np.sum(wgts)\n",
    "                super().__init__(dataset=dataset, **kwargs) \n",
    "\n",
    "        def get_idxs(self):\n",
    "            if self.n==0: return []\n",
    "            if not self.shuffle: return super().get_idxs()\n",
    "            return list(np.random.choice(self.n, self.n, p=self.wgts))\n",
    "    return WeightedDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = WeightedDL_Factory(TfmdDL)([1,2]*5,bs=10,wgts=[2,1]*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13333333, 0.06666667, 0.13333333, 0.06666667, 0.13333333,\n",
       "       0.06666667, 0.13333333, 0.06666667, 0.13333333, 0.06666667])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 2, 1, 2, 2, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.one_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ItemLst Transform for weight calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def uniform_p_of_category(cat_name):  \n",
    "    '''Scales sampling weights for an even distribution between every category'''\n",
    "    def _inner(df):\n",
    "        counts = df[cat_name].value_counts()\n",
    "        sample_prob =  1/counts\n",
    "        sample_prob.name = 'p_sample'\n",
    "        return df.merge(sample_prob,left_on=cat_name,right_index=True)\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(df):   \n",
    "    ''' test function that extracts valid and train from the path string'''\n",
    "    df['train'] = df.path.astype(str).str.contains('train',regex=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>train</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "      <th>p_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.001887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>1000</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.001887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>3001</td>\n",
       "      <td>0.001887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>3000</td>\n",
       "      <td>4001</td>\n",
       "      <td>0.001887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>4000</td>\n",
       "      <td>5001</td>\n",
       "      <td>0.001887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>False</td>\n",
       "      <td>260000</td>\n",
       "      <td>261001</td>\n",
       "      <td>0.003774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>False</td>\n",
       "      <td>261000</td>\n",
       "      <td>262001</td>\n",
       "      <td>0.003774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>False</td>\n",
       "      <td>262000</td>\n",
       "      <td>263001</td>\n",
       "      <td>0.003774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>False</td>\n",
       "      <td>263000</td>\n",
       "      <td>264001</td>\n",
       "      <td>0.003774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>False</td>\n",
       "      <td>264000</td>\n",
       "      <td>265001</td>\n",
       "      <td>0.003774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        path  train   l_slc   r_slc  p_sample\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   True       0    1001  0.001887\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   True    1000    2001  0.001887\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   True    2000    3001  0.001887\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   True    3000    4001  0.001887\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   True    4000    5001  0.001887\n",
       "..                                       ...    ...     ...     ...       ...\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5  False  260000  261001  0.003774\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5  False  261000  262001  0.003774\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5  False  262000  263001  0.003774\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5  False  263000  264001  0.003774\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5  False  264000  265001  0.003774\n",
       "\n",
       "[795 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm_lst = [train_valid, DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='current') ,uniform_p_of_category('train')]\n",
    "apply_df_tfms(get_hdf_files('test_data/'),tfm_lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,clm_shift=[-1,-1]),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,clm_shift=[1])),\n",
    "                 get_items=CreateDict(tfm_lst),\n",
    "                 splitter=ApplyToDict(ParentSplitter()))\n",
    "db = seq.dataloaders(get_hdf_files('test_data/'),dl_type=WeightedDL_Factory(TbpttDl),sub_seq_len=10,max_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00188679, 0.00188679, 0.00188679, 0.00188679, 0.00188679]),\n",
       " array([0.00377358, 0.00377358, 0.00377358, 0.00377358, 0.00377358]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.train.wgts[:5],db.valid.wgts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_model.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
