{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: dataloaders.html\n",
    "title: Truncated Backpropagation Through Time\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataloaders\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%matplotlib widget\n",
    "from fastai.callback.progress import *\n",
    "from fastai.callback.tracker import *\n",
    "from fastai.callback.schedule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from seqdata.core import *\n",
    "from seqdata.models.core import *\n",
    "from seqdata.learner import *\n",
    "from fastai.basics import *\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataloaders\n",
    "> Pytorch Modules for Training Models for sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tbptt dataloader needs to split the minibatches that are created in several smaller minibatches that will be returned sequentially before the next minibatch may be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.utils.data.dataloader import _MultiProcessingDataLoaderIter,_SingleProcessDataLoaderIter,_DatasetKind\n",
    "_loaders = (_MultiProcessingDataLoaderIter,_SingleProcessDataLoaderIter)\n",
    "\n",
    "@delegates()\n",
    "class TbpttDl(TfmdDL):\n",
    "\n",
    "    def __init__(self, dataset, sub_seq_len=None, seq_len = None ,shuffle=True,num_workers=2, **kwargs):\n",
    "#         assert sub_seq_len is not None\n",
    "        store_attr('sub_seq_len,seq_len')\n",
    "        self.rnn_reset = False\n",
    "        super().__init__(dataset=dataset, shuffle=shuffle, num_workers=num_workers, **kwargs)\n",
    "        \n",
    "    @property\n",
    "    def n_sub_seq(self):\n",
    "        if self.sub_seq_len is None: return 1\n",
    "        if self.seq_len is None: self.seq_len = self.do_item(0)[0].shape[0]\n",
    "        return math.ceil(self.seq_len / self.sub_seq_len)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return super().__len__() * self.n_sub_seq\n",
    "    \n",
    "    def _next_worker(self,w_id):\n",
    "        w_id += 1\n",
    "        if w_id > self.fake_l.num_workers-1: w_id = 0\n",
    "        return w_id\n",
    "\n",
    "    def sample(self):\n",
    "        #replaced new fastai sample formulation that store __idxs in main process\n",
    "        return (b for i,b in enumerate(self.__idxs) if i//(self.bs or 1)%self.num_workers==self.offs)\n",
    "#         return (b for i,b in enumerate(self.get_idxs()) if i//(self.bs or 1)%self.num_workers==self.offs)\n",
    "            \n",
    "    def __iter__(self):\n",
    "        '''iterator that handles multiprocessing by caching samples that are generated out of order'''\n",
    "        self.randomize()\n",
    "        self.before_iter()\n",
    "        self.__idxs=self.get_idxs() # called in context of main process (not workers/subprocesses)\n",
    "        \n",
    "        n_buffer = self.fake_l.num_workers*self.n_sub_seq\n",
    "        queue = {n:[] for n in range(self.fake_l.num_workers)} \n",
    "        current_worker = None\n",
    "        idx = 0\n",
    "        for loaded_b,w_id in _loaders[self.fake_l.num_workers==0](self.fake_l):\n",
    "\n",
    "            if w_id is None:\n",
    "                self.rnn_reset=True\n",
    "                b= loaded_b\n",
    "                self.rnn_reset = (idx % self.n_sub_seq) == 0\n",
    "                yield self.after_batch(b if self.device is None else to_device(b, self.device))\n",
    "                idx += 1 #idx increments after every yield, not every loop\n",
    "            else:\n",
    "                if current_worker is None:\n",
    "                    current_worker = w_id\n",
    "                \n",
    "                #retrieve queued elements from worker\n",
    "                while len(queue[current_worker]) > 0:\n",
    "                    b = queue[current_worker].pop(0)\n",
    "                    self.rnn_reset = (idx % self.n_sub_seq) == 0\n",
    "                    yield self.after_batch(b if self.device is None else to_device(b, self.device))\n",
    "                    idx += 1\n",
    "                    if (idx % self.n_sub_seq) == 0:\n",
    "                        current_worker = self._next_worker(current_worker) #next worker, stay in loop for the queue\n",
    "                        \n",
    "                \n",
    "                #retrieve fresh elements from worker\n",
    "                if w_id != current_worker: #not active worker\n",
    "                    queue[w_id] += [loaded_b]\n",
    "                    continue\n",
    "                else:#active worker\n",
    "                    b = loaded_b\n",
    "                    self.rnn_reset = (idx % self.n_sub_seq) == 0\n",
    "                    yield self.after_batch(b if self.device is None else to_device(b, self.device))\n",
    "                    idx += 1 #idx increments after every yield, not every loop\n",
    "                    if (idx % self.n_sub_seq) == 0:\n",
    "                        current_worker = self._next_worker(current_worker)\n",
    "                \n",
    "        self.after_iter()\n",
    "        if hasattr(self, 'it'): del(self.it)\n",
    "    \n",
    "    def create_batches(self, samps):\n",
    "        yield from self._tbptt_generator(super().create_batches(samps))\n",
    "        \n",
    "    def _tbptt_generator(self,batch_iter):\n",
    "        '''generator function that splits batches in smaller windows, yields mini_batch and worker id'''\n",
    "        for b in batch_iter:\n",
    "            for i in range(self.n_sub_seq):\n",
    "                #it is importan to retain the tuple type, or future transforms may now work\n",
    "                if self.sub_seq_len is None:\n",
    "                    trunc_b = b\n",
    "                else:\n",
    "                    trunc_b = tuple([retain_type(x[:,i*self.sub_seq_len:(i+1)*self.sub_seq_len],x) for x in b])\n",
    "                yield trunc_b, (None if torch.utils.data.get_worker_info() is None else torch.utils.data.get_worker_info().id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_lst = [DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='current')]\n",
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,clm_shift=[-1,-1]),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,clm_shift=[1])),\n",
    "                 get_items=CreateDict(tfm_lst),\n",
    "                 splitter=ApplyToDict(ParentSplitter()))\n",
    "db = seq.dataloaders(get_hdf_files('test_data/'),dl_type=TbpttDl,sub_seq_len=100,num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [array(x[-1][0,:,0].cpu()) for x in db.train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f6c710b3f248f3a3f4c3b22c377004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.concatenate(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_workers has to be 0. If there are parallel workers, the order of minibatches will be corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBPTT_Reset_Callback\n",
    "The stateful model needs to reset its hidden state, when a new sequence begins. The callback reads the reset flag and acts accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reset_model_state(model):\n",
    "    for m in model.modules():\n",
    "        if hasattr(m,'reset_state'): m.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TbpttResetCB(Callback):\n",
    "    \"`Callback` resets the rnn model with every new sequence for tbptt, calls `reset_state` in every module of the model\"\n",
    "        \n",
    "    def before_batch(self):\n",
    "        dl = self.learn.dls.train if self.training else self.learn.dls.valid\n",
    "#         if not self.training: import pdb; pdb.set_trace()\n",
    "        if (hasattr(dl,'rnn_reset') and dl.rnn_reset) or not hasattr(dl,'rnn_reset'):\n",
    "            reset_model_state(self.learn.model)\n",
    "        \n",
    "    def after_fit(self): \n",
    "        reset_model_state(self.learn.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrn = RNNLearner(db,num_layers=1,rnn_type='gru',stateful=False,metrics=[SkipNLoss(fun_rmse,1),fun_rmse])\n",
    "lrn.add_cb(TbpttResetCB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.478674</td>\n",
       "      <td>0.202241</td>\n",
       "      <td>0.307665</td>\n",
       "      <td>0.449460</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrn.fit_one_cycle(1,lr_max=3e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.train.sub_seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.015098</td>\n",
       "      <td>0.015470</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrn.fit_one_cycle(1,lr_max=3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Sampling Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weighted sampling dataloader for nonuniforly distributed data. A factory method receives the base Dataloader class and returns the inherited weighted sampling dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def WeightedDL_Factory(cls):\n",
    "    '''\n",
    "    Weighted Dataloader that provides control over sampling probabilities.\n",
    "    wgts: probability array with probability for every item\n",
    "            gets extracted from the pandas 'p_sample' column if given. \n",
    "            Otherwise uniform sampling will be enabled\n",
    "        \n",
    "    '''\n",
    "    assert issubclass(cls, TfmdDL)\n",
    "    \n",
    "    class WeightedDL(cls):\n",
    "        def __init__(self, dataset, wgts=None, **kwargs):\n",
    "#             import pdb;pdb.set_trace()\n",
    "            self.wgts = None\n",
    "            #self.items need to be assigned, but super.init needs wgts allready assigned\n",
    "            super().__init__(dataset=dataset, **kwargs) \n",
    "            if wgts is None:\n",
    "                if (isinstance(self.items,pd.DataFrame) and\n",
    "                    len(self.items) > 0 and \n",
    "                    'p_sample' in self.items):\n",
    "                    self.wgts = self.items.p_sample.to_numpy()\n",
    "                    self.wgts = self.wgts/self.wgts.sum()\n",
    "                elif (isinstance(self.items,Iterable) and\n",
    "                    len(self.items) > 0 and \n",
    "                    hasattr(self.items[0],'keys') and \n",
    "                    'p_sample' in self.items[0].keys()):\n",
    "                    self.wgts = np.array([x['p_sample'] for x in self.items])\n",
    "                    self.wgts = self.wgts/self.wgts.sum()\n",
    "                else:\n",
    "                    print('No wgts provided for WeightedDL. Was that intentional?')\n",
    "            else:\n",
    "                self.wgts = wgts/np.sum(wgts)\n",
    "\n",
    "        def get_idxs(self):\n",
    "            if self.n==0: return []\n",
    "            if not self.shuffle or self.wgts is None: return super().get_idxs()\n",
    "            #calculate number of elements with length of the dataset, for batch truncation\n",
    "            idxs = list(np.random.choice(self.n, size=len(self)*self.bs, p=self.wgts))\n",
    "            return idxs\n",
    "    return WeightedDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = WeightedDL_Factory(TfmdDL)([1,2]*5,bs=10,wgts=[2,1]*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13333333, 0.06666667, 0.13333333, 0.06666667, 0.13333333,\n",
       "       0.06666667, 0.13333333, 0.06666667, 0.13333333, 0.06666667])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 2, 1, 2, 1, 2, 1, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.one_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ItemLst Transform for weight calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def uniform_p_of_category(cat_name):  \n",
    "    '''Scales sampling weights for an even distribution between every category'''\n",
    "    def _inner(df):\n",
    "        if 'p_sample' in df:\n",
    "            df_targ = df.drop('p_sample',axis='columns')\n",
    "        else:\n",
    "            df_targ = df\n",
    "            \n",
    "        counts = df_targ[cat_name].value_counts()\n",
    "        sample_prob =  1/counts\n",
    "        sample_prob.name = 'p_sample'\n",
    "        df_res = df_targ.merge(sample_prob,left_on=cat_name,right_index=True)\n",
    "        \n",
    "        if 'p_sample' in df: \n",
    "            df_res.p_sample = df_res.p_sample* df.p_sample.values\n",
    "            \n",
    "        df_res.p_sample /= df_res.p_sample.sum()\n",
    "            \n",
    "        return df_res\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def uniform_p_of_float(var_name,bins = 10):\n",
    "    '''Scales sampling weights for an even distribution of the continous variable by creating equi sized bins'''\n",
    "    def _inner(df):\n",
    "        if 'p_sample' in df:\n",
    "            df_targ = df.drop('p_sample',axis='columns')\n",
    "        else:\n",
    "            df_targ = df\n",
    "            \n",
    "        df_targ['bins'] = pd.cut(df_targ[var_name], bins)\n",
    "        counts = df_targ['bins'].value_counts()\n",
    "        sample_prob =  1/counts\n",
    "        sample_prob.name = 'p_sample'\n",
    "        df_res = df_targ.merge(sample_prob,left_on='bins',right_index=True)\n",
    "        df_res.drop(['bins'],axis='columns',inplace=True)\n",
    "        \n",
    "        if 'p_sample' in df: \n",
    "            df_res.p_sample = df_res.p_sample* df.p_sample.values\n",
    "            \n",
    "        df_res.p_sample /= df_res.p_sample.sum()\n",
    "        \n",
    "        return df_res\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def uniform_p_of_float_with_gaps(var_name,bins = 100):\n",
    "    '''Scales sampling weights for an even distribution of the continous variable by creating equi sized bins'''\n",
    "    def _inner(df):\n",
    "        if 'p_sample' in df:\n",
    "            df_targ = df.drop('p_sample',axis='columns')\n",
    "        else:\n",
    "            df_targ = df\n",
    "            \n",
    "        l = df_targ[var_name].max()-df_targ[var_name].min() #value range\n",
    "        df_targ['bins'] = pd.qcut(df_targ[var_name],bins,duplicates='drop') #bins with rougly the same size\n",
    "        df_targ['p_sample'] =  df_targ['bins'].apply(lambda x: x.length).astype('f8')/l #sample_prob by bin width\n",
    "        sample_prob =  1/df_targ['bins'].value_counts() #correct uneven bin distribution\n",
    "        sample_prob.name = 'p_sample_correction'\n",
    "        df_res = df_targ.merge(sample_prob,left_on='bins',right_index=True)\n",
    "        \n",
    "        df_res.p_sample *= df_res.p_sample_correction\n",
    "        df_res.drop(['bins','p_sample_correction'],axis='columns',inplace=True)\n",
    "\n",
    "        if 'p_sample' in df: \n",
    "            df_res.p_sample = df_res.p_sample* df.p_sample.values\n",
    "            \n",
    "        df_res.p_sample /= df_res.p_sample.sum()\n",
    "        \n",
    "        return df_res\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(df):   \n",
    "    ''' test function that extracts valid and train from the path string'''\n",
    "    df['train'] = df.path.astype(str).str.contains('train',regex=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>train</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "      <th>p_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>False</td>\n",
       "      <td>1000</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>False</td>\n",
       "      <td>2000</td>\n",
       "      <td>3001</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>1000</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>False</td>\n",
       "      <td>264000</td>\n",
       "      <td>265001</td>\n",
       "      <td>0.000931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>263000</td>\n",
       "      <td>264001</td>\n",
       "      <td>0.000931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>264000</td>\n",
       "      <td>265001</td>\n",
       "      <td>0.000931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>263000</td>\n",
       "      <td>264001</td>\n",
       "      <td>0.000931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>True</td>\n",
       "      <td>264000</td>\n",
       "      <td>265001</td>\n",
       "      <td>0.000931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        path  train   l_slc   r_slc  p_sample\n",
       "0   test_data/valid/Sim_RealisticCycle3.hdf5  False       0    1001  0.001242\n",
       "0   test_data/valid/Sim_RealisticCycle3.hdf5  False    1000    2001  0.001242\n",
       "0   test_data/valid/Sim_RealisticCycle3.hdf5  False    2000    3001  0.001242\n",
       "1   test_data/train/Sim_RealisticCycle1.hdf5   True       0    1001  0.001242\n",
       "1   test_data/train/Sim_RealisticCycle1.hdf5   True    1000    2001  0.001242\n",
       "..                                       ...    ...     ...     ...       ...\n",
       "0   test_data/valid/Sim_RealisticCycle3.hdf5  False  264000  265001  0.000931\n",
       "1   test_data/train/Sim_RealisticCycle1.hdf5   True  263000  264001  0.000931\n",
       "1   test_data/train/Sim_RealisticCycle1.hdf5   True  264000  265001  0.000931\n",
       "2   test_data/train/Sim_RealisticCycle2.hdf5   True  263000  264001  0.000931\n",
       "2   test_data/train/Sim_RealisticCycle2.hdf5   True  264000  265001  0.000931\n",
       "\n",
       "[795 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "tfm_lst = [train_valid, DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='current') ,uniform_p_of_category('train'),uniform_p_of_float('l_slc'),uniform_p_of_float_with_gaps('r_slc')]\n",
    "apply_df_tfms(get_hdf_files('test_data/'),tfm_lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,clm_shift=[-1,-1]),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,clm_shift=[1])),\n",
    "                 get_items=CreateDict(tfm_lst),\n",
    "                 splitter=ApplyToDict(ParentSplitter()))\n",
    "# db = seq.dataloaders(get_hdf_files('test_data/'),dl_type=TbpttDl,sub_seq_len=200)\n",
    "db = seq.dataloaders(get_hdf_files('test_data/'),dl_type=WeightedDL_Factory(TbpttDl),sub_seq_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(530,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.train.wgts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0018654, 0.0018654, 0.0018654, 0.0018654, 0.0018654]),\n",
       " array([0.00371483, 0.00371483, 0.00371483, 0.00557225, 0.00557225]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.train.wgts[:5],db.valid.wgts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.754180</td>\n",
       "      <td>0.773359</td>\n",
       "      <td>0.839243</td>\n",
       "      <td>0.850851</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrn = RNNLearner(db,num_layers=1,rnn_type='gru',stateful=False,metrics=[SkipNLoss(fun_rmse,1),fun_rmse])\n",
    "lrn.add_cb(TbpttResetCB())\n",
    "lrn.fit_one_cycle(1,lr_max=3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch Limiter Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weighted sampling dataloader for nonuniforly distributed data. A factory method receives the base Dataloader class and returns the inherited weighted sampling dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def BatchLimit_Factory(cls):\n",
    "    '''\n",
    "    Batch limited Dataloader that provides an upper limit for the number of mini batches per epoch\n",
    "    max_batches: upper limit for minibatch count per epoch\n",
    "        \n",
    "    '''\n",
    "    assert issubclass(cls, TfmdDL)\n",
    "    \n",
    "    class BatchLimitDL(cls):\n",
    "        def __init__(self, dataset, max_batches=None, **kwargs):\n",
    "            self.max_batches = max_batches\n",
    "            # kwargs['n'] = max_batches*kwargs['bs'] n has to remain the full size, in order to create all indices if shuffled\n",
    "            super().__init__(dataset=dataset, **kwargs)\n",
    "\n",
    "        def __len__(self):\n",
    "            l = super().__len__() \n",
    "            if self.max_batches is not None: l = min(l,self.max_batches)\n",
    "            return l\n",
    "\n",
    "        def __iter__(self):\n",
    "            if self.max_batches is None: \n",
    "                yield from super().__iter__()\n",
    "            else:\n",
    "                for idx,b in enumerate(super().__iter__()):\n",
    "                    if idx >= self.max_batches: break\n",
    "                    yield b\n",
    "                    \n",
    "        #shuffle function that is called in super().get_idxs, truncated for faster execution\n",
    "        def shuffle_fn(self, idxs): return self.rng.sample(idxs, min(len(self)*self.bs,len(idxs)))\n",
    "                    \n",
    "        #get_idxs is truncated for the non-shuffling case, otherwise shuffle_fn is already truncated\n",
    "        def get_idxs(self):\n",
    "            return super().get_idxs()[:len(self)*self.bs]\n",
    "\n",
    "    return BatchLimitDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = BatchLimit_Factory(TfmdDL)([1,2]*5,bs=2,max_batches=3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.get_idxs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9, 2, 5, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BatchLimit_Factory(TfmdDL)([1,2]*5,bs=2,max_batches=3,shuffle=True).get_idxs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 2]), tensor([1, 2]), tensor([1, 2])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 01a_IndRNN.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "from nbdev.export import *\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
